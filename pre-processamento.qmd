<style>
.red-text {
  color: red;
}
.blue-text {
  color: blue;
}
</style>

# Pré-processamento

Antes de criarmos um modelo de predição, é importante plotarmos as variáveis do nosso modelo antecipadamente para observarmos se há algum comportamento estranho entre elas. Por exemplo, podemos ter uma variável que assuma frequentemente um único valor (possui muito pouca variabilidade), o que não acrescenta informações relevantes ao modelo, ou uma que possua alguns dados faltantes (NA's). O que podemos fazer nesses casos, que é o que iremos estudar neste capítulo, é realizar alterações em tais variáveis, afim de melhorar/otimizar a nossa predição/classificação. Essa é a ideia de **pré-processar**.

## Padronizando os dados

Vamos carregar o banco de dados spam e criar amostras treino e teste.

```{r}
library(kernlab)
library(caret)
data(spam)
set.seed(123)
noTreino = createDataPartition(y = spam$type, p = 0.75, list = F)
treino = spam[noTreino,]
teste = spam[-noTreino,]
# Vamos olhar para a variável capitalAve (média de letras maiúsculas por linha):
hist(treino$capitalAve,
     ylab = "Frequência",
     xlab = "Média de Letras Maiúsculas por Linha",
     main = "Histograma da Média de Letras Maiúsculas por Linha",
     col="steelblue", breaks = 4)
```

Podemos notar que muitos elementos estão próximos do 0 e os outros estão muito espalhados. Ou seja, essa variável não está trazendo muita informação para o modelo.

```{r}
mean(treino$capitalAve)
```

```{r}
sd(treino$capitalAve)
```

Podemos ver que a média é pequena mas o desvio padrão é muito grande.

Para que os algoritmos de machine learning não sejam enganados pelo fato de a variável ser altamente variável, vamos realizar um pré-processamento. Vamos padronizar os dados da variável pela amostra treino pegando cada valor dela e subtraindo pela sua média e dividindo pelo seu desvio padrão.

```{r}
treinoCapAve = treino$capitalAve
# Padronizando a variável:
treinoCapAveP = (treino$capitalAve-mean(treinoCapAve))/sd(treinoCapAve)
# Média da variável padronizada:
mean(treinoCapAveP)
```

Agora temos média 0.

```{r}
# Desvio padrão da variável padronizada:
sd(treinoCapAveP)
```

E variância 1.

```{r}
# Vamos olhar para a variável capitalAve (média de letras maiúsculas por linha):
hist(treinoCapAveP, ylab = "Frequência", xlab = "Média de Letras Maiúsculas por Linha",
     main = "Histograma da Média de Letras Maiúsculas por Linha",col="steelblue", breaks =4)
```

Agora vamos aplicar a mesma transformação na amostra teste. Uma coisa a ter em mente é que ao aplicar um algoritmo no conjunto de teste, só podemos usar os parâmetros que estimamos no conjunto de treino. **Ou seja, temos que usar a média e o desvio padrão da variável capitalAve do TREINO**.

```{r}
testeCapAve = teste$capitalAve
# Aplicando a transformação:
testeCapAveP = (testeCapAve-mean(treinoCapAve))/sd(treinoCapAve)
# Média da variável transformada do conjunto teste:
mean(testeCapAveP)
```

```{r}
# Desvio Padrão da variável transformada do conjunto teste:
sd(testeCapAveP)
```

Nesse caso não obtemos média 0 e variância 1, afinal nós utilizamos os parâmetros do treino para a padronização. Mas podemos notar que os valores estão relativamente próximos disso.

## Padronizando os dados com a função [PreProcess()]{.blue-text}

Podemos realizar o pré-processamento utilizando a função preProcess() do caret. Ela realiza vários tipos de padronizações, mas para utilizarmos a mesma (subtrair a média e dividir pelo desvio padrão) utilizamos o método c("center","scale").

```{r}
padronizacao = preProcess(treino, method = c("center","scale"))
# O comando acima cria um modelo de padronização. Para ter efeito ele deve ser aplicado nos dados com o
# comando predict().
treinoCapAveS = predict(padronizacao,treino)$capitalAve
# Média da variável padronizada:
mean(treinoCapAveS)
```

```{r}
# Desvio padrão da variável padronizada:
sd(treinoCapAveS)
```

Note que chegamos à mesma média e variância de quando padronizamos sem o preProcess().

Agora vamos aplicar essa padronização no conjunto de teste:

```{r}
testeCapAveS = predict(padronizacao,teste)$capitalAve
# Note que aplicamos o modelo de padronização criado com a amostra treino.
```

Observe que também encontramos o mesmo valor da média e desvio padrão de quando padronizamos a variável do conjunto teste anteriormente (sem o preProcess()):

```{r}
mean(testeCapAveS)
```

```{r}
sd(testeCapAveS)
```

Repare que também chegamos à mesma média e variância de quando padronizamos sem o preProcess().

## preProcess como argumento da função train()

Também podemos utilizar o preProcess dentro da função train da seguinte forma:

```{r warning=FALSE}
modelo = train(type~., data = treino, preProcess = c("center","scale"), 
               method = "glm")
```

A única limitação é que esse método aplica a padronização em **todas** as variáveis numéricas.

**Obs.:** Quando for padronizar uma variável da sua base para depois treinar seu algoritmo, lembre-se que colocar a variável padronizada de volta na sua base.

## Tratando NA's

É muito comum encontrar alguns dados faltantes (NA's) em uma base de dados. E quando você usa essa base para fazer predições, o algoritmo preditor muitas vezes falha, pois eles são criados para não manipular dados ausentes (na maioria dos casos). O mais recomendado a se fazer é descartar esses dados, principalmente se o número de variáveis for muito pequeno. Porém, em alguns casos, podemos tentar substituir os NA's da amostra por dados de outros elementos que possuam características parecidas.

**Obs:** Este é um procedimento que deve ser feito com muito cuidado, apenas em situações de real necessidade.

## Método *k-Nearest Neighbors (knn)*

O método k-Nearest Neighbors (knn) consiste em procurar os k vizinhos mais próximos do elemento que possui o dado faltante de uma variável de interesse, calculando a média dos valores observados dessa variável dos k vizinhos e imputando esse valor ao elemento.

Vamos utilizar novamente a variável capitalAve do banco de dados spam como exemplo.

```{r}
library(kernlab)
library(caret)
data(spam)
set.seed(13343)
# Criando amostras treino e teste:
noTreino = createDataPartition(y = spam$type, p = 0.75, list = F)
treino = spam[noTreino,]
teste = spam[-noTreino,]
```

Originalmente, a variável capitalAve não possui NA's. Mas para o objetivo de compreendermos como esse método funciona, vamos inserir alguns valores NA's.

```{r}
NAs = rbinom(dim(treino)[1], size = 1, p = 0.05)==1
```

O que fizemos com a função rbinom() é criar uma amostra de tamanho "dim(treino)\[1\]" (quantidade de elementos no treino) de uma variável Bernoulli com probabilidade de sucesso = 0,05. Ou seja, o vetor NAs será um vetor do tipo logical, onde será TRUE se o elemento gerado pela rbinom() é "1" (probabilidade de 0,05 de acontecer) e FALSE se é "0" (probabilidade 0,95 de acontecer).

Para preservar os valores originais, vamos criar uma nova coluna de dados no treino chamada capAve, que será uma réplica da variável capitalAve, mas com os NA's inseridos em alguns valores.

```{r}
library(dplyr)

# Criando a nova variável capAve com os mesmos valores da capitalAve:
treino = treino %>% mutate(capAve = capitalAve)

# Inserindo os Na's:
treino$capAve[NAs] = NA 
```

Então, recapitulando: criamos manualmente uma base de dados que possui valores faltantes. Agora podemos aplicar o método KNN para imputar valores aos NA's, escolhendo essa opção por meio do argumento "method" da função preProcess(). Na vida real, obviamente, não vamos criar dados faltantes, mas é possível que nossas bases tenham esses NA's e vamos ter que preenchê-los de alguma forma. O padrão da função é utilizar k=5 (número de vizinhos mais próximos igual a cinco).

```{r}
imput = preProcess(treino, method = "knnImpute")

# Aplicando o modelo de pré-processamento ao banco de dados treino:
treino$capAve = predict(imput,treino)$capAve

# Olhando para a variável capAve após o pré-processamento:
head(treino$capAve, n = 20)
```

Note que além de ter imputado valores aos NA's, o comando knnImpute também padronizou os dados.

**Obs:** O método knnImpute só resolve os NA's quando os dados faltantes são NUMÉRICOS.

E se quiséssemos aplicar o método de imputar valores aos NA's em **todo** o conjunto de dados, e não só em apenas 1 variável? Também podemos fazer isso utilizando a função preProcess().

Vamos utilizar a base de dados "airquality", já disponível no R, como exemplo.

```{r}
base = airquality
head(base, n = 15)
```

Note que essa base possui alguns valores NA's em algumas variáveis.

```{r}
# Realizando o método KNN para imputar valores aos NA's:
imput = preProcess(base, method = "knnImpute")

# Aplicando o modelo em toda a base de dados:
nova_base = predict(imput, base)

# Vamos olhar para a nova base:
head(nova_base, n = 15)
```

Note que ela não possui mais NA's e todas as variáveis foram padronizadas.

## Utilizando Algoritmos de *Machine Learning* com o Pacote mlr

O pacote [**mlr**]{.red-text} fornece vários métodos de imputação para dados faltantes. Alguns desses métodos possuem técnicas padrões como, por exemplo, imputação por uma constante (uma constante fixa, a média, a mediana ou a moda) ou números aleatórios (da distribuição empírica dos dados em consideração ou de uma determinada família de distribuições). Para mais informações sobre como utilizar essas imputações padrões, consulte https://mlr.mlr-org.com/reference/imputations.html.

Entretanto, a principal vantagem desse pacote - que é o que abordaremos nessa seção - é a possibilidade de imputação dos valores faltantes de uma variável por meio de predições de um algoritmo de machine learning, utilizando como base as outras variáveis. Ou seja, além de aceitar valores faltantes de variáveis numéricas para a imputação, ele também aceita de variáveis categóricas.

Podemos observar todos os algoritmos de machine learning possíveis de serem utilizados nesse pacote através da função listLearners().

-   Para um problema de imputação de NA's de variáveis numéricas temos os seguintes métodos:

```{r warning=FALSE}
library(mlr)
knitr::kable(listLearners("regr", properties = "missings")["class"])
```

-   Para um problema de imputação de NA's de variáveis categóricas temos os seguintes métodos:

```{r warning = FALSE}
knitr::kable(listLearners("classif", properties = "missings")["class"])
```

Vamos utilizar o banco de dados "heart" para realizarmos a imputação de dados faltantes categóricos.

```{r}
library(caret)
library(readr)
library(dplyr)

heart = read_csv("Heart.csv")

# Verificando se a base "heart" possui valores NA's em alguma variável:
apply(heart, 2, function(x) any(is.na(x)))
```

Note que a base não possui dados faltantes. Para fins didáticos, vamos inserir alguns na variável "Thal".

```{r}
# Criando um novo banco de dados que possuirá NA's:
new.heart = as.data.frame(heart)

set.seed(133)
# Criando um vetor do tipo *logical*, onde será TRUE se o elemento gerado pela rbinom() é "1"
# (probabilidade de 0,1 de acontecer):
NAs = rbinom(dim(new.heart)[1], size = 1, p = 0.1)==1

# Inserindo os NA's na variável Thal:
new.heart$Thal[NAs] = NA 
new.heart$Thal
```

Agora vamos imputar categorias aos dados faltantes da variável Thal. Iremos fazer isso através da função impute(). O único problema é que possuímos variáveis do tipo character na base de dados, e a função não aceita esta classe nos dados.

```{r}
str(new.heart)
```

Vamos transformar essas categorias em fatores.

```{r}
new.heart = mutate_if(new.heart, is.character, as.factor)
```

Vamos separar os dados em treino e teste.

```{r}
set.seed(133)
noTreino = caret::createDataPartition(y = new.heart$HeartDisease, p = 0.75,
                                      list = F)
treino = new.heart[noTreino,]
teste = new.heart[-noTreino,]
```

Agora vamos imputar os dados no conjunto treino com a função impute().

Para isso passamos como argumento:

-   A base de dados que possui os valores faltantes;

-   A variável resposta do modelo, ou seja, a variável de interesse para predição. No nosso exemplo essa variável é a "HeartDisease", que indica se uma pessoa possui uma doença cardíaca;

-   Lista contendo o método de imputação para cada coluna do banco de dados. Como apenas temos NA's na variável "Thal", a lista só possuirá essa variável, seguida do método de imputação que desejamos para ela. Vamos utilizar o método de árvores de decisão ("rpart").

```{r}
treino = mlr::impute(treino, target = "HeartDisease",
                     cols = list(Thal = imputeLearner("classif.rpart")))
```

Essa função retorna uma lista de tamanho 2, onde primeiro se encontra a base de dados após a imputação dos valores e em seguida detalhes do método utilizado.

Vamos olhar para a variável após a imputação dos dados:

```{r}
treino$data[,"Thal"]
```

Para implementarmos esse algoritmo no conjunto de dados teste basta utilizarmos a função reimpute() que implementaremos o mesmo método com os mesmos critérios criados no conjuno treino. Basta passar os seguintes argumentos:

-   A base de dados que possui os valores faltantes;

-   O mesmo método utilizado no treino.

A função retorna a base de dados com os valores imputados.

```{r}
teste = reimpute(teste, treino$desc)
teste$Thal
```

## Variável *Dummy*

As variáveis dummies ou variáveis indicadoras são formas de agregar informações qualitativas em modelos estatísticos. Ela atribui 1 se o elemento possui determinada característica, ou 0 caso ele não possua. Esse tipo de transformação é importante para modelos de regressão pois ela torna possível trabalhar com variáveis qualitativas.

Vamos utilizar o banco de dados Wage, do pacote ISLR. Este banco possui informações sobre 3000 trabalhadores do sexo masculino de uma região dos EUA, como por exemplo idade (age), tipo de trabalho (jobclass), salário (wage), entre outras. Nosso objetivo é tentar prever o salário do indivíduo em função das outras variáveis.

```{r}
library(ISLR)
data(Wage)
head(Wage)
```

Vamos olhar para 2 variáveis: jobclass (tipo de trabalho) e health_ins (indica se o trabalhador possui plano de saúde).

```{r}
library(ggplot2)
Wage %>% ggplot(aes(x=jobclass)) + geom_bar(aes(fill=jobclass)) +
  ylab("Frequência") + guides(fill=F) + theme_light() +
  ggtitle("Gráfico de Barras para o Tipo de Trabalho")
```

```{r}
Wage %>% ggplot(aes(x=health_ins)) + geom_bar(aes(fill=health_ins)) +
  ylab("Frequência") + guides(fill=F) + theme_light() +
  ggtitle("Gráfico de Barras para o Plano de Saúde")
```

Vamos transformar essas 2 variáveis em dummies por meio da função dummyVars().

```{r}
dummies = dummyVars(wage~jobclass+health_ins, data = Wage)

# Aplicando ao modelo:
Dummies = predict(dummies, newdata = Wage)

head(Dummies)
```

Note que ele transforma cada categoria numa variável dummy. Ou seja, como temos 2 categorias para jobclass, cada uma delas vira uma variável dummy, cujos nomes são "jobclass.1 Industrial" e "jobclass.2 Information". Então se para um indivíduo temos um "1" na categoria "jobclass=industrial", isso implica que terá um "0" na categoria "jobclass=information", pois ou o indivíduo tem um tipo de trabalho, ou tem outro. O mesmo vale para as categorias de plano de saúde.

Vale frisar que se separamos nossa base de dados em Treino e Teste, usar essa técnica de dummyVars vai criar as dummies considerando somente as classes que estão presentes na base de treino. Se na base de teste existirem outras classes possíveis que não estão presentes na base de treino, essas classes precisam ser especificadas na variável da base de treino com a função "factor". Assim, o algoritmo vai saber que essa classe também precisa de uma variável dummy.

Observe também que esse novo modelo criado é uma matriz:

```{r}
class(Dummies)
```

Vamos anexar esse novo objeto aos dados:

```{r}
Wage_dummy = cbind(Wage, Dummies)
head(Wage_dummy)
```

```{r}
# Removendo as variáveis categóricas do banco de dados completo (opcional):
Wage_dummy = dplyr::select(Wage_dummy, -c(jobclass,health_ins))
head(Wage_dummy)
```

Como comentado acima, nós temos uma variável dummy para cada categoria. Como tínhamos 2 variáveis qualitativas, então ficamos com 4 variáveis dummies. Porém, para um modelo de regressão, isso não é necessário. Estaríamos inserindo 2 variáveis com colinearidade perfeita no modelo: jobclass=industrial é totalmente correlacionada com jobclass=information, pois o resultado de uma influencia totalmente o da outra (o mesmo vale para as variáveis do plano de saúde). Dessa forma, vamos remover essas variáveis desnecessárias.

```{r}
Wage_dummy = dplyr::select(Wage_dummy, -c("jobclass.2. Information","health_ins.2. No"))
head(Wage_dummy)
```

Uma maneira mais simples de fazer isso, sem precisarmos retirar cada variável "na mão", é utilizar o argumento "fullRank=T" da função dummyVars().

```{r}
dummies = dummyVars(wage~jobclass+health_ins, data = Wage, fullRank = T)

# Aplicando ao modelo:
Dummies = predict(dummies, newdata = Wage)
```

Note que o comando fullRank=T removeu a primeira variável de cada classificação.

```{r}
# Anexando esse novo objeto aos dados:
Wage_dummy = cbind(Wage, Dummies)
head(Wage_dummy)
```

## Variância Zero ou Quase-Zero

Algumas vezes em um conjunto de dados podemos ter uma variável que assuma somente um único valor para todos os indivíduos, ou seja, ela possui variância zero. Ou podemos ter uma com uma frequência muito alta de um único valor, possuindo, assim, variância quase zero. Essas variáveis não auxiliam na predição, pois possuem o mesmo valor em muitos indivíduos, trazendo, assim, pouca informação ao modelo. Nosso objetivo é, então, identificar essas variáveis, chamadas de near zero covariates, para que possamos removê-las do nosso modelo de predição.

Para detectar as near zero covariates, utilizamos a função nearZeroVar() do pacote caret. Vamos verificar se há near zero covariates no banco de dados "forestfires".

Na função nearZeroVar() passamos primeiro a base de dados a ser analisada, depois o argumento lógico "saveMetrics", o qual se for "TRUE" retorna todos os detalhes sobre as variáveis da base de dados afim de identificar as near zero covariates. A saída da função fica da seguinte forma:

```{r}
library(readr)
library(caret)
incendio = read_csv("forestfires.csv")
nearZeroVar(incendio, saveMetrics = T)
```

Note que é retornado uma tabela onde nas linhas se encontram as variáveis da base de dados e as colunas podemos resumir da seguinte forma:

-   1ª coluna: a Taxa de Frequência de cada variável. Essa taxa é calculada pela razão de frequências do valor mais comum sobre o segundo valor mais comum.

-   2ª coluna: a Porcentagem de Valores Únicos. Ela é calculada utilizando o número de valores distintos sobre o número de amostras.

-   3ª coluna: indica se a variável tem variância zero.

-   4ª coluna: indica se a variável tem variância quase zero.

Podemos observar que a variável "rain" possui variância quase zero, portanto ela é uma near zero covariate.

```{r}
hist(incendio$rain, main = "Histograma da Variável Rain",
     xlab = "Variável Rain", ylab = "Frequência", col = "purple")
```

Logo, vamos excluir ela da nossa base de dados. O argumento "saveMetrics=FALSE" (default da função) retorna justamente qual(is) variável(is) do bando de dados é(são) near zero covariate .

```{r}
nzv = nearZeroVar(incendio)
Nova_incendio = incendio[,-nzv]
head(Nova_incendio)
```

## Análise de Componentes Principais (*PCA*)

Muitas vezes podemos ter variáveis em excesso no nosso banco de dados, o que torna difícil a manipulação das mesmas. A ideia geral do PCA (Principal Components Analysis) é reduzir a quantidade de variáveis, obtendo combinações interpretáveis delas. O PCA faz isso tranformando um conjunto de observações de variáveis possivelmente correlacionadas num conjunto de valores de variáveis linearmente não correlacionadas, chamadas de componentes principais. O número de componentes principais é sempre menor ou igual ao número de variáveis originais, e eles são selecionados de forma que expliquem uma alta porcentagem da variância do modelo.

Para utilizarmos o PCA no nosso modelo, basta colocar o argumento [preProcess="pca"]{.red-text} na função train(). Por padrão, são selecionadas componentes que expliquem 95% da variância.

Vamos aplicar o método "glm", com a opção "pca", no banco de dados spam.

```{r}
library(caret)
library(kernlab)
data(spam)
# Criando amostras treino/teste.
set.seed(36)
noTreino = createDataPartition(spam$type, p=0.75, list=F)
treino = spam[noTreino,] 
teste = spam[-noTreino,]
```

Agora vamos treinar o nosso modelo com o PCA.

```{r warning = FALSE}
set.seed(887)
modelo = caret::train(type ~ ., method = "glm", preProcess = "pca", data = treino)

# Aplicando o modelo na amostra TESTE:
testePCA = predict(modelo, teste)
```

Avaliando nosso modelo com a matriz de confusão:

```{r}
confusionMatrix(teste$type, testePCA)
```

O modelo obteve uma acurácia de 0,93, o que pode-se considerar uma alta taxa de acerto.

É possível alterar a porcentagem de variância a ser explicada pelos componentes nas opções do train().

Por exemplo, vamos alterar a porcentagem da variância para 60%.

```{r warning = FALSE}
controle = trainControl(preProcOptions = list(thresh = 0.6))

# Treinando o modelo 2:
set.seed(754)
modelo2 = caret::train(type ~ ., method = "glm", preProcess = "pca", data = treino, trControl = controle)

# Aplicando o modelo 2:
testePCA2 = predict(modelo2, teste)
```

Avaliando o segundo modelo pela matriz de confusão:

```{r}
confusionMatrix(teste$type,testePCA2)
```

Obtemos uma acurácia de 0,92, o que indica também uma alta taxa de acerto, porém um pouco menor que a do modelo anterior. Note que a sensitividade e a especificidade também diminuíram.

Em geral, são utilizados pontos de corte para a variãncia explicada acima de 0,9.

## *PCA* fora da função train()

Podemos também realizar o pré-processamento fora da função train(). Primeiramente vamos criar o pré-processamento, utilizando a amostra treino.

```{r}
PCA = preProcess(treino, method = c("center","scale","pca"), thresh = 0.95)
```

**Obs:** pode-se fixar o número de componentes, utilizando o argumento "pcaComp" ao invés de "thresh".

Agora aplicamos o pré-processamento na amostra treino e realizamos o treinamento, utilizando a amostra treino já pré-processada.

```{r warning = FALSE}
treinoPCA = predict(PCA, treino)
modelo = caret::train(type ~ ., data = treinoPCA, method="glm")
```

Aplicando o pré-processamento na amostra teste:

```{r}
testePCA = predict(PCA, teste)
```

Por último, aplicamos o modelo criado com a amostra treino na amostra teste pré-processada.

```{r}
testeMod = predict(modelo, testePCA)

# Avaliando o modelo:
confusionMatrix(testePCA$type, testeMod)
```

## Normalização dos Dados

### Transformação de Box-Cox

A transformação de Box-Cox é uma transformação feita nos dados (contínuos) para tentar normalizá-los. Considerando $X_1, X_2, ..., X_n$ as variáveis do conjunto de dados original, essa transformação consiste em encontrar um $\lambda$ tal que as variáveis transformadas $Y_1, Y_2, ..., Y_n$ se aproximem de uma distribuição normal com variância constante.

Essa transformação é dada pela seguinte forma: $Y_i = \frac{X_i^\lambda-1}{\lambda}$, se $\lambda \neq 0$. O parâmetro $\lambda$ é estimado utilizando o método de máxima verossimilhança.

O método de Box-Cox é o método mais simples e o mais eficiente computacionalmente. Podemos aplicar a transformação de Box-Cox nos dados através da função preProcess().

**Obs:** a transformação de Box-Cox só pode ser utilizada com dados positivos.

```{r}
treinoBC = preProcess(treino, method = "BoxCox") 
```

### Outras Transformações

#### Transformação de Yeo-Johnson

A transformação de Yeo-Johnson é semelhante à transformação de Box-Cox, porém ela aceita preditores com dados nulos e/ou dados negativos. Também podemos aplicá-la aos dados através da função preProcess().

```{r}
treinoYJ = preProcess(treino, method = "YeoJohnson")
```

#### Transformação Exponencial de Manly

O método exponencial de Manly também consiste em estimar um $\lambda$ tal que as variáveis transformadas se aproximem de uma distribuição normal. Assim como a transformação de Yeo-Johnson, ela também aceita dados positivos, nulos e negativos. Essa transformação é dada pela seguinte forma:

$Y_i = \frac{e^{X\lambda}-1}{\lambda}$, se \$\lambda \neq 0}.

```{r warning = FALSE}
treinoEXP = preProcess(treino, method = "expoTrans")
```
