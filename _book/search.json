[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "APRENDIZADO DE MÁQUINA COM R",
    "section": "",
    "text": "Bem-vindo\nEste curso tem como objetivo propagar as ideias básicas de aprendizado de máquina e previsão no *software* estatístico R. A ideia principal é cobrir as técnicas mais usadas como regressão linear, árvores de decisão, e também detalhes básicos e aspectos práticos do aprendizado de máquina. Inicialmente será utilizado alguns códigos básicos do R para alguns modelos de previsão. Contudo, o foco principal será no pacote caret, o qual tem a finalidade de tornar as técnicas de aprendizado mais simples, combinando um grande número de preditores que foram construídos no R."
  },
  {
    "objectID": "index.html#pré-requisitos",
    "href": "index.html#pré-requisitos",
    "title": "APRENDIZADO DE MÁQUINA COM R",
    "section": "Pré-requisitos",
    "text": "Pré-requisitos\nOs pré-requisitos que serão úteis para o curso são: análise exploratória de dados no R, programação básica em R e conhecimentos teóricos básicos sobre modelos de regressão."
  },
  {
    "objectID": "intro.html#o-que-é-o-aprendizado-de-máquina",
    "href": "intro.html#o-que-é-o-aprendizado-de-máquina",
    "title": "Introdução",
    "section": "O que é o Aprendizado de Máquina?",
    "text": "O que é o Aprendizado de Máquina?\nEm 1959, Arthur Samuel definiu o aprendizado de máquina como o “campo de estudo que dá aos computadores a habilidade de aprender sem serem explicitamente programados”. Ou seja, é um método de análise de dados que automatiza a construção de modelos analíticos. É baseado na ideia de que sistemas podem aprender com dados, identificar padrões e tomar decisões com o mínimo de intervenção humana. A importância desse aprendizado se deve principalmente ao fato de que atualmente tem surgido cada vez mais a necessidade de manipulações de grandes volumes e variedades de dados disponíveis."
  },
  {
    "objectID": "intro.html#para-que-serve",
    "href": "intro.html#para-que-serve",
    "title": "Introdução",
    "section": "Para que serve?",
    "text": "Para que serve?\nCom o aprendizado de máquina é possível produzir, rápida e automaticamente, modelos capazes de analisar dados maiores e mais complexos, e entregar resultados mais rápidos e precisos – mesmo em grande escala."
  },
  {
    "objectID": "intro.html#onde-é-usado",
    "href": "intro.html#onde-é-usado",
    "title": "Introdução",
    "section": "Onde é usado?",
    "text": "Onde é usado?\nAo construir modelos precisos há mais chances de identificar boas oportunidades e de evitar riscos desconhecidos. Na prática, podemos citar alguns exemplos reais do uso de aprendizado de máquina:\n\nOs governos locais podem tentar prever os pagamentos de pensão no futuro para que eles saibam se seus mecanismos de geração de receita têm fundos suficientes gerados para cobrir esses pagamentos de pensão.\nO Google pode querer prever se você vai clicar em um anúncio para que ele possa mostrar apenas os anúncios com maior probabilidade de receber cliques e, assim, aumentar a receita.\nA Amazon, a Netflix e outras empresas como essa mostram um filme e querem que você veja um próximo filme. Para fazer isso, eles querem mostrar a você o que você pode estar interessado, para que eles possam mantê-lo assistindo e, novamente, aumentar a receita.\nAs seguradoras empregam grandes grupos de atuários e estatísticos para tentar prever seu risco de todo tipo de coisas diferentes, como por exemplo a morte."
  },
  {
    "objectID": "intro.html#como-funciona",
    "href": "intro.html#como-funciona",
    "title": "Introdução",
    "section": "Como funciona?",
    "text": "Como funciona?\nA funcionalidade do aprendizado de máquina se resume a tentar prever um certo modelo para o conjunto de dados em questão. Há dois modos de isso ser feito: pelo aprendizado supervisionado e pelo aprendizado não supervisionado. Veremos a definição de cada um deles a seguir."
  },
  {
    "objectID": "tipos-am.html#aprendizado-não-supervisionado",
    "href": "tipos-am.html#aprendizado-não-supervisionado",
    "title": "Tipos de Aprendizado de Máquina",
    "section": "Aprendizado não supervisionado",
    "text": "Aprendizado não supervisionado\nNa aprendizagem não supervisionada, temos um conjunto de dados não rotulados e queremos de alguma forma agrupá-los por um certo padrão encontrado. Vejamos alguns exemplos:\n\nExemplo 1: Dada uma imagem de homem/mulher, temos de prever sua idade com base em dados da imagem.\nExemplo 2: Dada as informações sobre que músicas uma pessoa costuma ouvir, sugerir outras que possam agradá-la também."
  },
  {
    "objectID": "tipos-am.html#aprendizado-supervisionado",
    "href": "tipos-am.html#aprendizado-supervisionado",
    "title": "Tipos de Aprendizado de Máquina",
    "section": "Aprendizado supervisionado",
    "text": "Aprendizado supervisionado\nNo aprendizado supervisionado, por outro lado, temos um conjunto de dados já rotulados que sabemos qual é a nossa saída correta e que deve ser semelhante ao conjunto. Queremos assim, com base nesses dados, ser capaz de classificar outros dados do mesmo tipo e que ainda não foram rotulados.\n\nExemplo 1: Dada uma coleção de 1000 pesquisas de uma universidade, encontrar uma maneira de agrupar automaticamente estas pesquisas em grupos que são de alguma forma semelhantes ou relacionadas por diferentes variáveis, tais como a frequência das palavras, frases, contagem de páginas, etc.\nExemplo 2: Dada uma grande amostra de e-mails, encontrar uma maneira de agrupá-los automaticamente em “spam” ou “não spam”, de acordo com as características das palavras, tais como a frequência com que uma certa palavra aparece, a frequência de letras maiúsculas, de cifrões ($), entre outros.\n\nSe os valores da variável rótulo, também chamada de variável de interesse, são valores discretos finitos ou ainda categóricos, então temos um problema de classificação e o algoritmo que criaremos para resolver nosso problema será chamado Classificador.\nSe os valores da Variável de Interesse são valores contínuos, então temos um problema de regressão e o algoritmo que criaremos será chamado Regressor.\nA aprendizagem supervisionada será o principal foco do curso."
  },
  {
    "objectID": "pred.html#pergunta",
    "href": "pred.html#pergunta",
    "title": "Predição",
    "section": "Pergunta",
    "text": "Pergunta\nO nosso objetivo é responder a uma pergunta de tipo “O dado A é do tipo x ou do tipo y?”. Por exemplo, podemos querer saber se é possível detectar automaticamente se um e-mail é um spam ou um “ham”, isto é, não spam. O que na verdade queremos saber é: “É possível usar características quantitativas para classificar um e-mail como spam?”."
  },
  {
    "objectID": "pred.html#amostra-de-entrada",
    "href": "pred.html#amostra-de-entrada",
    "title": "Predição",
    "section": "Amostra de Entrada",
    "text": "Amostra de Entrada\nUma vez formulada a pergunta, precisamos obter uma amostra de onde tentaremos extrair informações que caracterizam a categoria a qual um dado pertence e então usar essas informações para classificar outros dados não categorizados. O ideal é que se tenha uma amostra grande, assim teremos melhores parâmetros para construir nosso preditor.\nNo caso da pergunta sobre um e-mail ser spam ou não, temos acesso a base de dados “spam” disponível no pacote “kernlab”, onde cada linha dessa base é um e-mail e nas colunas temos a porcentagem de palavras e números contidos em cada e-mail e, entre outras coisas, a nossa variável de interesse “type” que classifica o e-mail como spam ou não:\n\nlibrary(kernlab)\ndata(spam)\nhead(spam)\n\n  make address  all num3d  our over remove internet order mail receive will\n1 0.00    0.64 0.64     0 0.32 0.00   0.00     0.00  0.00 0.00    0.00 0.64\n2 0.21    0.28 0.50     0 0.14 0.28   0.21     0.07  0.00 0.94    0.21 0.79\n3 0.06    0.00 0.71     0 1.23 0.19   0.19     0.12  0.64 0.25    0.38 0.45\n4 0.00    0.00 0.00     0 0.63 0.00   0.31     0.63  0.31 0.63    0.31 0.31\n5 0.00    0.00 0.00     0 0.63 0.00   0.31     0.63  0.31 0.63    0.31 0.31\n6 0.00    0.00 0.00     0 1.85 0.00   0.00     1.85  0.00 0.00    0.00 0.00\n  people report addresses free business email  you credit your font num000\n1   0.00   0.00      0.00 0.32     0.00  1.29 1.93   0.00 0.96    0   0.00\n2   0.65   0.21      0.14 0.14     0.07  0.28 3.47   0.00 1.59    0   0.43\n3   0.12   0.00      1.75 0.06     0.06  1.03 1.36   0.32 0.51    0   1.16\n4   0.31   0.00      0.00 0.31     0.00  0.00 3.18   0.00 0.31    0   0.00\n5   0.31   0.00      0.00 0.31     0.00  0.00 3.18   0.00 0.31    0   0.00\n6   0.00   0.00      0.00 0.00     0.00  0.00 0.00   0.00 0.00    0   0.00\n  money hp hpl george num650 lab labs telnet num857 data num415 num85\n1  0.00  0   0      0      0   0    0      0      0    0      0     0\n2  0.43  0   0      0      0   0    0      0      0    0      0     0\n3  0.06  0   0      0      0   0    0      0      0    0      0     0\n4  0.00  0   0      0      0   0    0      0      0    0      0     0\n5  0.00  0   0      0      0   0    0      0      0    0      0     0\n6  0.00  0   0      0      0   0    0      0      0    0      0     0\n  technology num1999 parts pm direct cs meeting original project   re  edu\n1          0    0.00     0  0   0.00  0       0     0.00       0 0.00 0.00\n2          0    0.07     0  0   0.00  0       0     0.00       0 0.00 0.00\n3          0    0.00     0  0   0.06  0       0     0.12       0 0.06 0.06\n4          0    0.00     0  0   0.00  0       0     0.00       0 0.00 0.00\n5          0    0.00     0  0   0.00  0       0     0.00       0 0.00 0.00\n6          0    0.00     0  0   0.00  0       0     0.00       0 0.00 0.00\n  table conference charSemicolon charRoundbracket charSquarebracket\n1     0          0          0.00            0.000                 0\n2     0          0          0.00            0.132                 0\n3     0          0          0.01            0.143                 0\n4     0          0          0.00            0.137                 0\n5     0          0          0.00            0.135                 0\n6     0          0          0.00            0.223                 0\n  charExclamation charDollar charHash capitalAve capitalLong capitalTotal type\n1           0.778      0.000    0.000      3.756          61          278 spam\n2           0.372      0.180    0.048      5.114         101         1028 spam\n3           0.276      0.184    0.010      9.821         485         2259 spam\n4           0.137      0.000    0.000      3.537          40          191 spam\n5           0.135      0.000    0.000      3.537          40          191 spam\n6           0.000      0.000    0.000      3.000          15           54 spam\n\n\nObtida a amostra, precisamos dividi-la em duas partes que chamaremos de Conjunto de Treino e Conjunto de Teste. O conjunto de treino será usado para construir o algoritmo. É dele que vamos extrair as informações que julgarmos utéis para classificar uma categoria de dado. É importante que o modelo de previsão seja feito com base apenas no conjunto de treino.\n\nset.seed(127)\nindices = sample(dim(spam)[1], size = 2760)\ntreino = spam[indices,]\nteste = spam[-indices,]\n\nApós construido o algoritmo, usaremos o conjunto de teste para obter a estimativa de erro, que será detalhada mais a frente."
  },
  {
    "objectID": "pred.html#características",
    "href": "pred.html#características",
    "title": "Predição",
    "section": "Características",
    "text": "Características\nTemos que encontrar agora características que possam indicar a categoria dos dados. Podemos, por exemplo, vizualizar algumas variáveis graficamente para obter uma ideia do que podemos fazer. No nosso exemplo de e-mails, podemos querer avaliar se a frequência de palavras “your” em um e-mail pode indicar se ele é um spam ou não.\n\nplot(density(treino$your[treino$type==\"nonspam\"]), col=\"blue\", \n     main = \"Densidade de 'your' em ham (azul) e spam (vermelho)\", \n     xlab = \"Frequência de 'your'\", ylab = \"densidade\")\nlines(density(treino$your[treino$type==\"spam\"]), col=\"red\")\n\n\n\n\nPelo gráfico podemos notar que a maioria dos e-mails que são spam têm uma frequência maior da palavra “your”. Por outro lado, aqueles que são classificados como ham (não spam) têm um pico mais alto perto do 0."
  },
  {
    "objectID": "pred.html#algoritmo",
    "href": "pred.html#algoritmo",
    "title": "Predição",
    "section": "Algoritmo",
    "text": "Algoritmo\nCom base nisso podemos construir um algoritmo para prever se um e-mail é spam ou ham. Podemos estimar um modelo onde queremos encontrar uma constante c tal que se a frequência da palavra “your” for maior que c, então classificamos o e-mail como spam. Caso contrário, classificamos o e-mail como não spam.\nVamos observar graficamente como ficaria esse modelo se c=0.8.\n\nplot(density(treino$your[treino$type==\"nonspam\"]), col=\"blue\", \n     main = \"Densidade de 'your' em ham (azul) e spam (vermelho)\", \n     xlab = \"Frequência de 'your'\", ylab = \"densidade\")\nlines(density(treino$your[treino$type==\"spam\"]), col=\"red\")\nabline(v=0.8,col=\"black\")\n\n\n\n\nOs e-mails à direita da linha preta seriam classificados como spam, enquanto que os à esquerda seriam classificados como não spam.\n\nAvaliação\nAgora vamos avaliar nosso modelo de predição.\n\npredicao=ifelse(treino$your&gt;0.8,\"spam\",\"nonspam\")\ntable(predicao,treino$type)/length(treino$type)\n\n         \npredicao    nonspam      spam\n  nonspam 0.4978261 0.1293478\n  spam    0.1155797 0.2572464\n\n\nPodemos ver que quando os e-mails não eram spam e classificamos como “não spam”, de acordo com nosso modelo, em 50% do tempo nós acertamos. Quando os e-mails eram spam e classificamos ele em spam, por volta de 26% do tempo nós acertamos. Então, ao total, nós acertamos por volta de 50+26=76% do tempo. Então nosso algoritmo de previsão tem uma precisão por volta de 76% na amostra treino.\n\npredicao=ifelse(teste$your&gt;0.8,\"spam\",\"nonspam\")\ntable(predicao,teste$type)/length(teste$type)\n\n         \npredicao    nonspam      spam\n  nonspam 0.4910375 0.1434003\n  spam    0.1037480 0.2618142\n\n\nJá na amostra teste acertamos 48+27=75% das vezes. O erro na amostra teste é o que chamamos de erro real. É o erro que esperamos em amostras novas que passarem por nosso preditor.\n\n\nComo construir um bom algoritmo de aprendizado de máquina?\nO “melhor” método de aprendizado de máquina é caracterizado por:\n\nUma boa base de dados;\nReter informações relevantes;\nSer bem interpretável;\nFácil de ser explicado e entendido;\nSer preciso;\nFácil de se construir e de se testar em pequenas amostras;\nFácil aplicar a um grande conjunto de dados.\n\nOs erros mais comuns, que se deve tomar um certo cuidado, são:\n\nTentar automatizar a seleção de variáveis (características) de uma maneira que não permita que você entenda como essas variáveis estão sendo aplicadas para fazer previsões;\nNão prestar atenção a peculiaridades específicas de alguns dados, como comportamentos estranhos de variáveis específicas;\nJogar fora informações desnecessariamente."
  },
  {
    "objectID": "design-pred.html",
    "href": "design-pred.html",
    "title": "4  Design de predição",
    "section": "",
    "text": "1. Defina sua taxa de erro (benchmark).\nPor hora iremos utilizar uma taxa de erro genérica, mas em um próximo iremos falar sobre quais são as diferentes taxas de erro possíveis que você pode escolher.\nPor exemplo, podemos calcular o chamado erro majoritário que é o limite máximo abaixo do qual o erro de um classificador deve estar. Ele é dado por 1−p, onde p é a proporção da categoria mais requente na variável de interesse. Por exemplo, se a variável de interesse possui 2 categorias: A e B. Se 85% dos dados estão rotulados na categoria A e 15% na categoria B, entao temos que a categoria A é a classe majoritária e 100%−85%=15% é o erro majoritário.\nCaso o erro do preditor seja superior ao erro majoritário, seria melhor classificar toda nova amostra na classe majoritária, certo? Depende.\nDigamos que um psicólogo quer construir um classificador para prever se uma pessoa tem ou não ideação suicida, ou seja, pensa ou planeja suicídio. Suponha que ele tem uma base de dados com 1000 observações cuja variável de interesse “Tem ideação suicida?” está rotulada com “sim” ou “não” e 97% das observações, no caso indivíduos/pacientes, não possuem tal característica e portanto 3% dos indivíduos possuem. Criado o preditor, observamos que o erro é de 5%, assim como mostrado a seguir:\n\nAs partes em vermelho mostram o erro cometido por ambos os métodos. Agora note as pessoas que possuem ideação suicída porém foram classficadas como não possuidoras dessa característica. Quanto isso afetará no dignóstico do psicólogo?\n2. Divida os dados em Treino e Teste, ou Treino, Teste e Validação (opcional).\nComo já comentado, o conjunto de treino deve ser criado para construir seu modelo e o conjunto de testes para avaliar seu modelo. Fazemos isso com o intuito de criarmos um modelo que se ajuste bem a qualquer base de dados, e não apenas à nossa. É comum usar 70% da amostra como treino e 30% como teste, mas isso não é uma regra. Podemos também repartir os dados em treino, teste e validação(*). É importante ficar claro que quem está conduzindo as análises é quem fica encarregado de decidir o que fica melhor para cada amostra.\n3. Definimos quais variáveis serão utilizadas para estimação dos parâmetros do classificador/regressor (função preditora).\nNem sempre utilizar todas as variáveis do banco de dados é importante para o modelo. Pode acontecer de termos variáveis que não ajudam na predição, como por exemplo aquelas com uma variância quase zero (frequência muito alta de um único valor). Iremos estudar algumas formas de selecionar as melhores variáveis para o modelo em breve.\n4. Definimos o método que será utilizado para construção do classificador/regressor.\nIsso poderá ser feito, por exemplo, utilizando o método de validação cruzada (cross-validation), que será explicado detalhadamente em um capítulo mais à frente.\n5. Obtenção do melhor modelo.\nUtilizando a amostra TREINO, definimos os parâmetros da função preditora (classificador/regressor), obtendo o modelo final.\n6. Aplicamos o modelo final na amostra TESTE (uma única vez), para estimar o erro do preditor.\nAplicamos o modelo final obtido na amostra de teste apenas uma vez. Se aplicarmos várias vezes até encontrar o melhor modelo, estaremos, de certa forma, utilizando a amostra de teste para treinar o modelo, o que influenciaria o ajuste do modelo com base nos resultados do teste. Isso não é desejável, pois o objetivo da amostra de teste é servir como uma “nova amostra” para estimar a taxa de erro do modelo.\n(*) Opcionalmente poderá ser criado um conjunto de validação, com o intuito de servir como um “pré-teste”, que também será usado para avaliar seu modelo. Quando repartimos o conjunto de dados dessa forma, utilizamos o treino para construir o modelo, avaliamos o modelo na validação (ou seja, o ajuste do modelo é influenciado por ela), e se o resultado não for bom, retornamos ao treino para ajustar um outro modelo. Então novamente testamos o modelo na validação, e assim sucessivamente até acharmos um modelo que se adequou bem tanto ao treino quanto à validação. Aí, finalmente, aplicamos ele ao conjunto teste, avaliando na prática a sua qualidade.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Design de predição</span>"
    ]
  },
  {
    "objectID": "erros-amostrais.html#erro-dentro-da-amostra-in-sample-error",
    "href": "erros-amostrais.html#erro-dentro-da-amostra-in-sample-error",
    "title": "Erros Amostrais",
    "section": "Erro dentro da Amostra (In Sample Error)",
    "text": "Erro dentro da Amostra (In Sample Error)\nÉ a taxa de erro que você recebe no mesmo conjunto de dados usado para criar seu preditor. Na literatura às vezes é chamado de erro de resubstituição. Em outras palavras, é quando seu algoritmo de previsão se ajusta ao que você coletou num conjunto de dados específico. E assim, quando você recebe um novo conjunto de dados, a precisão diminuirá."
  },
  {
    "objectID": "erros-amostrais.html#erro-fora-da-amostra-out-of-sample-error",
    "href": "erros-amostrais.html#erro-fora-da-amostra-out-of-sample-error",
    "title": "5  Erros Amostrais",
    "section": "5.2 Erro fora da Amostra (Out of Sample Error)",
    "text": "5.2 Erro fora da Amostra (Out of Sample Error)\nÉ a taxa de erro que você recebe em um novo conjunto de dados. Na literatura às vezes é chamado de erro de generalização. Uma vez que coletamos uma amostra de dados e construímos um modelo para ela, podemos querer testá-lo em uma nova amostra, por exemplo uma amostra coletada em um horário diferente ou em um local diferente. Daí podemos analisar o quão bem o algoritmo executará a predição nesse novo conjunto de dados.\n\n5.2.1 Algumas ideias-chave\n\nQuase sempre o erro fora da amostra é o que interessa.\nErro dentro da amostra é menor que o erro fora da amostra.\nUm erro frequente é ajustar muito o algoritmo aos dados que temos. Em outras palavras, criar um modelo sobreajustado (também chamado de overfitting(*)).\n(*) Overfitting é um termo usado na estatística para descrever quando um modelo estatístico se ajusta muito bem a um conjunto de dados anteriormente observado e, como consequência, se mostra ineficaz para prever novos resultados.\n\nVejamos um exemplo de erro dentro da amostra vs erro fora da amostra:\n\nset.seed(131)\n\n# Vamos selecionar as linhas da base de dados spam através de uma amostra de tamanho 10 das 4601 linhas\n# dos dados:\nspamMenor = spam[sample(dim(spam)[1], size = 10), ]\n\n# Vamos criar um vetor composto pelos rótulos \"1\" e \"2\". \n# Se um e-mail da nossa amostra for spam, recebe \"1\", se não for spam, recebe \"2\".\nspamRotulos = (spamMenor$type == \"spam\")*1 + 1\n\n# Na nossa base a variável capitalAve representa a média de letras maiúsculas por linha.\nplot(spamMenor$capitalAve, col = spamRotulos, xlab = \"Quantidade de Letras Maiúsculas\",\n     ylab = \"Frequência\", main = \"Letras Maiúsculas em spam (vermelho) e em ham (preto)\",\n     pch = 19)\n\n\n\n\n\n\n\n\nPodemos notar que, em geral, as mensagens classificadas como spam possuem uma frequência maior de letras maiúsculas do que as mensagens classificadas como não spam. Com base nisso queremos construir um preditor, onde podemos classificar e-mails como spam se a frequência de letras maiúsculas for maior que uma determida constante, e não spam caso contrário.\nVeja que se separarmos os dados pela frequência de letras maiúsculas maior que 2,5 e classificarmos o que está acima como spam e abaixo como não spam, ainda teríamos duas observações que não são spam acima da linha.\n\nplot(spamMenor$capitalAve, col = spamRotulos, xlab = \"Quantidade de Letras Maiúsculas\",\n     ylab = \"Frequência\", main = \"Letras Maiúsculas em spam (vermelho) e em ham (preto)\",\n     pch = 19)\nabline(h = 2.5, lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\nEntão o melhor para esse caso é criar o seguinte modelo:\n\nletras maiúsculas &gt; 2,5 e &lt; 3,8 ⇒ spam;\nletras maiúsculas &lt; 2,5 ou &gt; 3,8 ⇒ não spam.\n\nplot(spamMenor$capitalAve, col = spamRotulos, xlab = \"Quantidade de Letras Maiúsculas\",\n     ylab = \"Frequência\", main = \"Letras Maiúsculas em spam (vermelho) e em ham (preto)\",\n     pch = 19)\nabline(h = c(2.5, 3.8), lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n# construindo o modelo sobreajustado\nmodelo.sobreajustado = function(x){\n  predicao = rep(NA, length(x))\n  predicao[(x&gt;=2.5 & x&lt;=3.8)] = \"spam\"\n  predicao[(x&lt;2.5 | x&gt;3.8)] = \"nonspam\"\n  return(predicao)\n}\n# avaliando o modelo sobreajustado\nresultado = modelo.sobreajustado(spamMenor$capitalAve)\ntable(resultado, spamMenor$type)\n\n         \nresultado nonspam spam\n  nonspam       7    0\n  spam          0    3\n\n\nNote que obtivemos uma precisão perfeita nessa amostra, como já era esperado. Nesse caso, o erro dentro da amostra é de 0%. Mas será que esse modelo é o mais eficiente em outros dados também?\nVamos usar essa segunda regra para criarmos um modelo mais geral:\n\nletras maiúsculas &gt; 2,5 ⇒ spam;\nletras maiúsculas &lt;= 2,5 ⇒ não spam.\n\nplot(spamMenor$capitalAve, col = spamRotulos, xlab = \"Quantidade de Letras Maiúsculas\",\n     ylab = \"Frequência\", main = \"Letras Maiúsculas em spam (vermelho) e em ham (preto)\",\n     pch = 19)\nabline(h = 2.5, lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n# construindo o modelo geral\nmodelo.geral = function(x){\n  predicao = rep(NA, length(x))\n  predicao[x&gt;=2.5] = \"spam\"\n  predicao[x&lt;2.5] = \"nonspam\"\n  return(predicao)\n}\n# avaliando o modelo geral\nresultado2 = modelo.geral(spamMenor$capitalAve)\ntable(resultado2, spamMenor$type)\n\n          \nresultado2 nonspam spam\n   nonspam       5    0\n   spam          2    3\n\n\nObserve que dessa forma temos um erro dentro da amostra de 20%. Vamos agora aplicar esses dois modelos para toda a base de dados:\n\ntable(modelo.sobreajustado(spam$capitalAve), spam$type)\n\n         \n          nonspam spam\n  nonspam    2297 1385\n  spam        491  428\n\n\n\ntable(modelo.geral(spam$capitalAve), spam$type)\n\n         \n          nonspam spam\n  nonspam    2042  540\n  spam        746 1273\n\n\nOlhando para a precisão de nossos modelos:\n\nsum(modelo.sobreajustado(spam$capitalAve) == spam$type)\n\n[1] 2725\n\n\n\nsum(modelo.geral(spam$capitalAve) == spam$type)\n\n[1] 3315\n\n\nObserve que utilizando o modelo sobreajustado obtivemos um erro fora da amostra de 40,77%, enquanto que com o modelo geral esse erro foi de 27,95%. Note que se queremos construir um modelo que melhor representa qualquer amostra que pegarmos, um modelo não sobreajustado possuirá uma precisão maior.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Erros Amostrais</span>"
    ]
  },
  {
    "objectID": "caret.html",
    "href": "caret.html",
    "title": "6  Avaliando Preditores - Introdução ao Pacote Caret",
    "section": "",
    "text": "6.1 Avaliando Classificadores\nVamos utilizar a base de dados spam novamente para realizarmos o procedimento de predição para um e-mail (se ele é spam ou não spam), dessa vez utilizando o pacote caret.\nPara fazer a separação da amostra em treino e teste vamos primeiramente particionar a base de dados com a função createDataPartition().\nlibrary(caret)\nlibrary(kernlab)\ndata(spam)\nset.seed(371)\nnoTreino = createDataPartition(y = spam$type, p = 0.75, list = F)\nEssa função retorna os números das linhas a serem selecionadas para o treino. Os principais argumentos são:\nAgora vamos separar o que irá para o treino e o que irá para o teste.\n# Separando as linhas para o treino:\ntreino = spam[noTreino,]\n# Separando as linhas para o teste:\nteste = spam[-noTreino,]\nDado que já foi feito a separação das amostras treino e teste, o próximo passo é realizarmos o treinamento. Para isso é preciso escolher um dos modelos para ser utilizado. Uma lista com todos os modelos implementados no pacote caret pode ser vista com o seguinte comando:\nnames(getModelInfo())\n\n  [1] \"ada\"                 \"AdaBag\"              \"AdaBoost.M1\"        \n  [4] \"adaboost\"            \"amdai\"               \"ANFIS\"              \n  [7] \"avNNet\"              \"awnb\"                \"awtan\"              \n [10] \"bag\"                 \"bagEarth\"            \"bagEarthGCV\"        \n [13] \"bagFDA\"              \"bagFDAGCV\"           \"bam\"                \n [16] \"bartMachine\"         \"bayesglm\"            \"binda\"              \n [19] \"blackboost\"          \"blasso\"              \"blassoAveraged\"     \n [22] \"bridge\"              \"brnn\"                \"BstLm\"              \n [25] \"bstSm\"               \"bstTree\"             \"C5.0\"               \n [28] \"C5.0Cost\"            \"C5.0Rules\"           \"C5.0Tree\"           \n [31] \"cforest\"             \"chaid\"               \"CSimca\"             \n [34] \"ctree\"               \"ctree2\"              \"cubist\"             \n [37] \"dda\"                 \"deepboost\"           \"DENFIS\"             \n [40] \"dnn\"                 \"dwdLinear\"           \"dwdPoly\"            \n [43] \"dwdRadial\"           \"earth\"               \"elm\"                \n [46] \"enet\"                \"evtree\"              \"extraTrees\"         \n [49] \"fda\"                 \"FH.GBML\"             \"FIR.DM\"             \n [52] \"foba\"                \"FRBCS.CHI\"           \"FRBCS.W\"            \n [55] \"FS.HGD\"              \"gam\"                 \"gamboost\"           \n [58] \"gamLoess\"            \"gamSpline\"           \"gaussprLinear\"      \n [61] \"gaussprPoly\"         \"gaussprRadial\"       \"gbm_h2o\"            \n [64] \"gbm\"                 \"gcvEarth\"            \"GFS.FR.MOGUL\"       \n [67] \"GFS.LT.RS\"           \"GFS.THRIFT\"          \"glm.nb\"             \n [70] \"glm\"                 \"glmboost\"            \"glmnet_h2o\"         \n [73] \"glmnet\"              \"glmStepAIC\"          \"gpls\"               \n [76] \"hda\"                 \"hdda\"                \"hdrda\"              \n [79] \"HYFIS\"               \"icr\"                 \"J48\"                \n [82] \"JRip\"                \"kernelpls\"           \"kknn\"               \n [85] \"knn\"                 \"krlsPoly\"            \"krlsRadial\"         \n [88] \"lars\"                \"lars2\"               \"lasso\"              \n [91] \"lda\"                 \"lda2\"                \"leapBackward\"       \n [94] \"leapForward\"         \"leapSeq\"             \"Linda\"              \n [97] \"lm\"                  \"lmStepAIC\"           \"LMT\"                \n[100] \"loclda\"              \"logicBag\"            \"LogitBoost\"         \n[103] \"logreg\"              \"lssvmLinear\"         \"lssvmPoly\"          \n[106] \"lssvmRadial\"         \"lvq\"                 \"M5\"                 \n[109] \"M5Rules\"             \"manb\"                \"mda\"                \n[112] \"Mlda\"                \"mlp\"                 \"mlpKerasDecay\"      \n[115] \"mlpKerasDecayCost\"   \"mlpKerasDropout\"     \"mlpKerasDropoutCost\"\n[118] \"mlpML\"               \"mlpSGD\"              \"mlpWeightDecay\"     \n[121] \"mlpWeightDecayML\"    \"monmlp\"              \"msaenet\"            \n[124] \"multinom\"            \"mxnet\"               \"mxnetAdam\"          \n[127] \"naive_bayes\"         \"nb\"                  \"nbDiscrete\"         \n[130] \"nbSearch\"            \"neuralnet\"           \"nnet\"               \n[133] \"nnls\"                \"nodeHarvest\"         \"null\"               \n[136] \"OneR\"                \"ordinalNet\"          \"ordinalRF\"          \n[139] \"ORFlog\"              \"ORFpls\"              \"ORFridge\"           \n[142] \"ORFsvm\"              \"ownn\"                \"pam\"                \n[145] \"parRF\"               \"PART\"                \"partDSA\"            \n[148] \"pcaNNet\"             \"pcr\"                 \"pda\"                \n[151] \"pda2\"                \"penalized\"           \"PenalizedLDA\"       \n[154] \"plr\"                 \"pls\"                 \"plsRglm\"            \n[157] \"polr\"                \"ppr\"                 \"pre\"                \n[160] \"PRIM\"                \"protoclass\"          \"qda\"                \n[163] \"QdaCov\"              \"qrf\"                 \"qrnn\"               \n[166] \"randomGLM\"           \"ranger\"              \"rbf\"                \n[169] \"rbfDDA\"              \"Rborist\"             \"rda\"                \n[172] \"regLogistic\"         \"relaxo\"              \"rf\"                 \n[175] \"rFerns\"              \"RFlda\"               \"rfRules\"            \n[178] \"ridge\"               \"rlda\"                \"rlm\"                \n[181] \"rmda\"                \"rocc\"                \"rotationForest\"     \n[184] \"rotationForestCp\"    \"rpart\"               \"rpart1SE\"           \n[187] \"rpart2\"              \"rpartCost\"           \"rpartScore\"         \n[190] \"rqlasso\"             \"rqnc\"                \"RRF\"                \n[193] \"RRFglobal\"           \"rrlda\"               \"RSimca\"             \n[196] \"rvmLinear\"           \"rvmPoly\"             \"rvmRadial\"          \n[199] \"SBC\"                 \"sda\"                 \"sdwd\"               \n[202] \"simpls\"              \"SLAVE\"               \"slda\"               \n[205] \"smda\"                \"snn\"                 \"sparseLDA\"          \n[208] \"spikeslab\"           \"spls\"                \"stepLDA\"            \n[211] \"stepQDA\"             \"superpc\"             \"svmBoundrangeString\"\n[214] \"svmExpoString\"       \"svmLinear\"           \"svmLinear2\"         \n[217] \"svmLinear3\"          \"svmLinearWeights\"    \"svmLinearWeights2\"  \n[220] \"svmPoly\"             \"svmRadial\"           \"svmRadialCost\"      \n[223] \"svmRadialSigma\"      \"svmRadialWeights\"    \"svmSpectrumString\"  \n[226] \"tan\"                 \"tanSearch\"           \"treebag\"            \n[229] \"vbmpRadial\"          \"vglmAdjCat\"          \"vglmContRatio\"      \n[232] \"vglmCumulative\"      \"widekernelpls\"       \"WM\"                 \n[235] \"wsrf\"                \"xgbDART\"             \"xgbLinear\"          \n[238] \"xgbTree\"             \"xyf\"\nPara o nosso exemplo vamos utilizar o “glm” (generalized linear model).\nAgora vamos criar o nosso modelo, utilizando apenas a amostra treino. Para isso vamos usar o comando train().\nmodelo = train(type ~ ., data = treino, method = \"glm\")\nNo primeiro argumento colocamos qual variável estamos tentando prever em função de qual(is). No nosso caso, queremos prever “type” em função (“~”) de todas as outras, por isso utilizamos o “.”. Em seguida dizemos de qual base de dados queremos construir o modelo e por último o método de treinamento utilizado.\nAgora vamos dar uma olhada no nosso modelo.\nmodelo\n\nGeneralized Linear Model \n\n3451 samples\n  57 predictor\n   2 classes: 'nonspam', 'spam' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 3451, 3451, 3451, 3451, 3451, 3451, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.9224544  0.8365594\nPodemos observar que utilizamos uma amostra de tamanho 3451 no treino e 57 preditores para prever a qual classe um e-mail pertence, spam ou não spam. O que a função faz é realizar várias maneiras diferentes de testar se esse modelo funcionará bem e usar isso para selecionar o melhor modelo. Neste caso ela usou a reamostragem por bootstrapping com 25 replicações (o default da função).\nUma vez que ajustamos o modelo podemos aplicá-lo na amostra teste, para estimarmos a precisão do classificador. Para isso utilizamos o comando predict(). Dentro da função nós passamos o modelo que ajustamos no treino e em qual base de dados gostaríamos de realizar a predição.\npredicao = predict(modelo, newdata = teste)\nhead(predicao, n=30)\n\n [1] spam    spam    spam    spam    spam    spam    spam    nonspam nonspam\n[10] spam    spam    spam    nonspam nonspam spam    spam    spam    spam   \n[19] spam    spam    spam    spam    spam    spam    spam    spam    spam   \n[28] spam    spam    spam   \nLevels: nonspam spam\nAo fazermos isso obtemos uma série de predições para as classes dos e-mails do conjunto teste. Podemos então realizar a avaliação do modelo comparando os resultados da predição com as reais classes dos e-mails, por meio do comando confusionMatrix().",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Avaliando Preditores - Introdução ao Pacote Caret</span>"
    ]
  },
  {
    "objectID": "caret.html#matriz-de-confusão-confusion-matrix",
    "href": "caret.html#matriz-de-confusão-confusion-matrix",
    "title": "Avaliando Preditores - Introdução ao Pacote Caret",
    "section": "Matriz de Confusão (Confusion Matrix)",
    "text": "Matriz de Confusão (Confusion Matrix)\nA matriz de confusão é a matriz de comparação feita após a predição, onde as linhas correspondem ao que foi previsto e as colunas correspondem à verdade conhecida.\nExemplo: A matriz de confusão para o problema de predição dos e-mails em spam ou não spam fica da seguinta forma:\n\nOnde na primeira coluna se encontram os elementos que possuem a característica de interesse (os e-mails que são spam), e, respectivamente nas linhas, os que foram corretamente identificados - o qual são chamados de Verdadeiros Positivos (VP) - e os que foram erroneamente identificados - os Falsos Negativos (FP). Na segunda coluna se encontram os elementos que não possuem a característica de interesse (os e-mails que são ham) e, respectivamente nas linhas, os que foram erroneamente identificados - o qual são chamados de Falsos Positivos (FN) - e os que foram corretamente identificados - os Verdadeiros Negativos (VN).\nCom as devidas classificações a matriz de confusão fica da seguinte forma:\n\nDentro da função passamos as predições que obtemos pelo modelo ajustado e as reais classificações dos e-mails do conjunto teste.\n\nconfusionMatrix(predicao, teste$type)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction nonspam spam\n   nonspam     659   55\n   spam         38  398\n                                          \n               Accuracy : 0.9191          \n                 95% CI : (0.9018, 0.9342)\n    No Information Rate : 0.6061          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8295          \n                                          \n Mcnemar's Test P-Value : 0.09709         \n                                          \n            Sensitivity : 0.9455          \n            Specificity : 0.8786          \n         Pos Pred Value : 0.9230          \n         Neg Pred Value : 0.9128          \n             Prevalence : 0.6061          \n         Detection Rate : 0.5730          \n   Detection Prevalence : 0.6209          \n      Balanced Accuracy : 0.9120          \n                                          \n       'Positive' Class : nonspam         \n                                          \n\n\nA função retorna a matriz de confusão e alguns dados estatísticos, como por exemplo a Precisão (Accuracy), o Intervalo de Confiança com 95% de confiança (95% CI), a Sensibilidade (Sensitivity), Especificidade (Specificity), entre outros.\nPodemos notar que o GLM foi um bom modelo de treinamento para os nossos dados pois obtivemos altas taxas de acertos: uma precisão de 0,94, 0,96 de sensitividade e 0,90 de especificidade. Vamos ver melhor algumas dessas estatísticas:\nDefinição (Sensibilidade): A sensibilidade de um método de predição é a porcentagem dos elementos da amostra que possuem a característica de interesse e foram corretamente identificados. Para o nosso exemplo dos e-mails, a sensabilidade é a porcentagem dos e-mails que são spam e foram classificados pelo nosso algoritmo de predição como spam.\nOu seja, podemos escrever \\(Sensibilidade = \\frac{VP}{VP+FN}\\)\nDefinição (Especificidade): A especificidade de um método de predição é a porcentagem dos elementos da amostra que não possuem a característica de interesse e foram corretamente identificados. Para o nosso exemplo dos e-mails, a especificidade é a porcentagem dos e-mails que são “ham” e o algoritmo de predição os classificou como tal.\nOu seja, podemos escrever \\(Especificidade = \\frac{VN}{VN+FP}\\)\nQuando obtemos as sensibilidades e as especificidades de diferentes preditores, naturalmente surge o questionamente: qual deles é melhor para estimar as verdadeiras características de interesse? A resposta depende do que é mais importante para o problema.\nSe identificar corretamente os positivos for mais importante, utilizamos o preditor com maior sensibilidade. Se identificar corretamente os negativos for mais importante, utilizamos o preditor com maior especificidade.\nOutra medida para avaliar a qualidade do nosso preditor é a precisão (Accuracy). Ela avalia a porcentagem de acertos que tivemos em geral. Ou seja, somamos o número de Verdadeiros Positivos com o número de Verdadeiros Negativos e dividimos pelo tamanho da amostra. \\(Precisão = \\frac{VP+VN}{VP+VN+FN+FP}\\)\nPara demais medidas da matriz de confusão consulte o [apêndice].\n\nAvaliando Regressores\nAgora vamos utilizar a base de dados faithful para tentar prever o tempo de espera (waiting) entre uma erupção e outra de um gêiser dado a duração das erupções (eruption).\n\ndata(\"faithful\")\nhead(faithful)\n\n  eruptions waiting\n1     3.600      79\n2     1.800      54\n3     3.333      74\n4     2.283      62\n5     4.533      85\n6     2.883      55\n\n\nPrimeiro, vamos separar a amostra em treino e teste.\n\nset.seed(39)\nnoTreino = createDataPartition(y=faithful$waiting, p=0.7, groups = 5, list=F)\ntreino = faithful[noTreino,]; teste = faithful[-noTreino,]\n\nQuando o argumento y é numérico, a amostra é dividida em grupos com base nos percentis e é feita uma amostragem estratificada. O número de percentis é definido pelo argumento groups.\nAgora temos que treinar nosso modelo. Para esse exemplo vamos usar a Regressão Linear (LM - Linear Regression).\n\nOs métodos disponíveis e seus usos podem ser encontrados no guia do caret.\n\nVamos treinar nosso modelo utilizando a amostra treino.\n\nmodelo = caret::train(waiting~eruptions, data = treino, method = \"lm\")\n\nNovamente, colocamos a variável que tentamos prever em função das outras. No caso, só temos duas variáveis então não precisamos colocar o ponto como no [classificador].\n\nmodelo\n\nLinear Regression \n\n192 samples\n  1 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 192, 192, 192, 192, 192, 192, ... \nResampling results:\n\n  RMSE      Rsquared  MAE     \n  6.060639  0.805468  4.948071\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nPodemos ver que temos 192 observações no conjunto treino e 1 preditor.\nAgora vamos aplicar nosso modelo na amostra teste para avaliar o erro dele.\n\npredicao = predict(modelo, newdata = teste)\n\nAssim como no classificador, a função predict nos retorna a previsão dos tempos entre as erupções dado os tempos das durações das erupções."
  },
  {
    "objectID": "caret.html#mse",
    "href": "caret.html#mse",
    "title": "Avaliando Preditores - Introdução ao Pacote Caret",
    "section": "MSE",
    "text": "MSE\nAssim como há diversas formas de compararmos a qualidade dos classificadores, há também diversas formas de compararmos regressores. O que estudaremos agora é o MSE (mean squared error - erro quadrático médio). Mais formas de comparação de regressores também serão vistas futuramente.\nO MSE é a média de quanto os valores previstos para as observações se distanciaram dos valores verdadeiros dessa observação. Obtemos ele somando essas distâncias entre os valores previstos e os reais ao quadrado e dividindo por n.\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^{n} \\left( Yreal_i - Yestimado_i \\right)^2\n\\]\n\nEx.: O erro quadrático médio para o problema de tempo de erupção do gêiser.\n\ndata(\"faithful\")\nhead(faithful)\n\n  eruptions waiting\n1     3.600      79\n2     1.800      54\n3     3.333      74\n4     2.283      62\n5     4.533      85\n6     2.883      55\n\n\n\n# Gráfico do tempo entre as erupções em função do tempo de erupção do gêiseres\nplot(faithful$eruptions, faithful$waiting, pch = 20, ylab=\"Tempo entre Erupções\",\n     xlab = \"Tempo de Erupção\", main = \"Tempo entre as erupções em função do tempo de erupção do gêiser\")\n\n\n\n\nPodemos notar que há uma relação linear positiva entre as variáveis. Vamos então ajustar um modelo de regressão linear.\n\nmodelo = lm(faithful$waiting~faithful$eruptions)\nplot(y = faithful$waiting, x = faithful$eruptions, pch = 20, ylab=\"Tempo entre erupções\",\n     xlab = \"Tempo de erupção\", main = \"Tempo entre as erupções em função do tempo de erupção do gêiser\")\nabline(modelo, col = \"red\", lwd = 2)\n\n\n\n\nNa reta de regressão temos todos os valores previstos para o tempo de erupção de acordo com os tempos de espera. Podemos então calcular o MSE para o nosso modelo utilizando o comando mse().\n\nmse = sum((teste$waiting-predicao)**2)/nrow(teste)\nmse\n\n[1] 32.41941\n\n\nEntão temos que, em média, o valor estimado para a variável de interesse no conjunto de teste se distancia do valor real observado em 32,41941 escores. Note que esta é uma medida que soma as distâncias ao quadrado, por isso o MSE é um número relativamente grande."
  },
  {
    "objectID": "teste-link.html",
    "href": "teste-link.html",
    "title": "APRENDIZADO DE MÁQUINA COM R",
    "section": "",
    "text": "Base Heart\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(downloadthis)\n\ndata = read.csv(\"/Users/luana.moreno/Documents/projeto-uff/Projeto de Pesquisa em Ciencia de Dados/Heart.csv\")\n\ndata %&gt;%\n  download_this(\n    output_name = \"Heart\",\n    output_extension = \".csv\",\n    button_label = \"Base Heart\",\n    button_type = \"primary\",\n    has_icon = TRUE,\n    icon = \"fa fa-save\"\n  )\n\n Base Heart"
  },
  {
    "objectID": "caret.html#avaliando-classificadores",
    "href": "caret.html#avaliando-classificadores",
    "title": "6  Avaliando Preditores - Introdução ao Pacote Caret",
    "section": "",
    "text": "y = classe dos dados que deverá ser mantida a mesma proporção nos conjuntos treino e teste. Para o nosso exemplo, escolhemos manter a mesma proporção do tipo do e-mail. Sendo assim, tanto no treino como no teste teremos a mesma proporção de e-mails spam e não spam.\np = porcentagem da amostra que será utilizada para o treino. Para o nosso exemplo, escolhemos 75%.\nlist = argumento do tipo logical, se TRUE → os resultados serão mostrados em uma lista, se FALSE → os resultados serão mostrados em uma matriz.\n\n\nOBS: Esse comando deve ser utilizado apenas quando os dados são amostras independentes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.1 Matriz de Confusão (Confusion Matrix)\nA matriz de confusão é a matriz de comparação feita após a predição, onde as linhas correspondem ao que foi previsto e as colunas correspondem à verdade conhecida.\nExemplo: A matriz de confusão para o problema de predição dos e-mails em spam ou não spam fica da seguinta forma:\n\nOnde na primeira coluna se encontram os elementos que possuem a característica de interesse (os e-mails que são spam), e, respectivamente nas linhas, os que foram corretamente identificados - o qual são chamados de Verdadeiros Positivos (VP) - e os que foram erroneamente identificados - os Falsos Negativos (FP). Na segunda coluna se encontram os elementos que não possuem a característica de interesse (os e-mails que são ham) e, respectivamente nas linhas, os que foram erroneamente identificados - o qual são chamados de Falsos Positivos (FN) - e os que foram corretamente identificados - os Verdadeiros Negativos (VN).\nCom as devidas classificações a matriz de confusão fica da seguinte forma:\n\nDentro da função passamos as predições que obtemos pelo modelo ajustado e as reais classificações dos e-mails do conjunto teste.\n\nconfusionMatrix(predicao, teste$type)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction nonspam spam\n   nonspam     659   55\n   spam         38  398\n                                          \n               Accuracy : 0.9191          \n                 95% CI : (0.9018, 0.9342)\n    No Information Rate : 0.6061          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8295          \n                                          \n Mcnemar's Test P-Value : 0.09709         \n                                          \n            Sensitivity : 0.9455          \n            Specificity : 0.8786          \n         Pos Pred Value : 0.9230          \n         Neg Pred Value : 0.9128          \n             Prevalence : 0.6061          \n         Detection Rate : 0.5730          \n   Detection Prevalence : 0.6209          \n      Balanced Accuracy : 0.9120          \n                                          \n       'Positive' Class : nonspam         \n                                          \n\n\nA função retorna a matriz de confusão e alguns dados estatísticos, como por exemplo a Precisão (Accuracy), o Intervalo de Confiança com 95% de confiança (95% CI), a Sensibilidade (Sensitivity), Especificidade (Specificity), entre outros.\nPodemos notar que o GLM foi um bom modelo de treinamento para os nossos dados pois obtivemos altas taxas de acertos: uma precisão de 0,94, 0,96 de sensitividade e 0,90 de especificidade. Vamos ver melhor algumas dessas estatísticas:\nDefinição (Sensibilidade): A sensibilidade de um método de predição é a porcentagem dos elementos da amostra que possuem a característica de interesse e foram corretamente identificados. Para o nosso exemplo dos e-mails, a sensabilidade é a porcentagem dos e-mails que são spam e foram classificados pelo nosso algoritmo de predição como spam.\nOu seja, podemos escrever \\(Sensibilidade = \\frac{VP}{VP+FN}\\)\nDefinição (Especificidade): A especificidade de um método de predição é a porcentagem dos elementos da amostra que não possuem a característica de interesse e foram corretamente identificados. Para o nosso exemplo dos e-mails, a especificidade é a porcentagem dos e-mails que são “ham” e o algoritmo de predição os classificou como tal.\nOu seja, podemos escrever \\(Especificidade = \\frac{VN}{VN+FP}\\)\nQuando obtemos as sensibilidades e as especificidades de diferentes preditores, naturalmente surge o questionamente: qual deles é melhor para estimar as verdadeiras características de interesse? A resposta depende do que é mais importante para o problema.\nSe identificar corretamente os positivos for mais importante, utilizamos o preditor com maior sensibilidade. Se identificar corretamente os negativos for mais importante, utilizamos o preditor com maior especificidade.\nOutra medida para avaliar a qualidade do nosso preditor é a precisão (Accuracy). Ela avalia a porcentagem de acertos que tivemos em geral. Ou seja, somamos o número de Verdadeiros Positivos com o número de Verdadeiros Negativos e dividimos pelo tamanho da amostra. \\(Precisão = \\frac{VP+VN}{VP+VN+FN+FP}\\)\nPara demais medidas da matriz de confusão consulte o [apêndice].",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Avaliando Preditores - Introdução ao Pacote Caret</span>"
    ]
  },
  {
    "objectID": "caret.html#avaliando-regressores",
    "href": "caret.html#avaliando-regressores",
    "title": "6  Avaliando Preditores - Introdução ao Pacote Caret",
    "section": "6.2 Avaliando Regressores",
    "text": "6.2 Avaliando Regressores\nAgora vamos utilizar a base de dados faithful para tentar prever o tempo de espera (waiting) entre uma erupção e outra de um gêiser dado a duração das erupções (eruption).\n\ndata(\"faithful\")\nhead(faithful)\n\n  eruptions waiting\n1     3.600      79\n2     1.800      54\n3     3.333      74\n4     2.283      62\n5     4.533      85\n6     2.883      55\n\n\nPrimeiro, vamos separar a amostra em treino e teste.\n\nset.seed(39)\nnoTreino = createDataPartition(y=faithful$waiting, p=0.7, groups = 5, list=F)\ntreino = faithful[noTreino,]; teste = faithful[-noTreino,]\n\nQuando o argumento y é numérico, a amostra é dividida em grupos com base nos percentis e é feita uma amostragem estratificada. O número de percentis é definido pelo argumento groups.\nAgora temos que treinar nosso modelo. Para esse exemplo vamos usar a Regressão Linear (LM - Linear Regression).\n\nOs métodos disponíveis e seus usos podem ser encontrados no guia do caret.\n\nVamos treinar nosso modelo utilizando a amostra treino.\n\nmodelo = caret::train(waiting~eruptions, data = treino, method = \"lm\")\n\nNovamente, colocamos a variável que tentamos prever em função das outras. No caso, só temos duas variáveis então não precisamos colocar o ponto como no [classificador].\n\nmodelo\n\nLinear Regression \n\n192 samples\n  1 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 192, 192, 192, 192, 192, 192, ... \nResampling results:\n\n  RMSE      Rsquared  MAE     \n  6.060639  0.805468  4.948071\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nPodemos ver que temos 192 observações no conjunto treino e 1 preditor.\nAgora vamos aplicar nosso modelo na amostra teste para avaliar o erro dele.\n\npredicao = predict(modelo, newdata = teste)\n\nAssim como no classificador, a função predict nos retorna a previsão dos tempos entre as erupções dado os tempos das durações das erupções.\n\n6.2.1 MSE\nAssim como há diversas formas de compararmos a qualidade dos classificadores, há também diversas formas de compararmos regressores. O que estudaremos agora é o MSE (mean squared error - erro quadrático médio). Mais formas de comparação de regressores também serão vistas futuramente.\nO MSE é a média de quanto os valores previstos para as observações se distanciaram dos valores verdadeiros dessa observação. Obtemos ele somando essas distâncias entre os valores previstos e os reais ao quadrado e dividindo por n.\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^{n} \\left( Yreal_i - Yestimado_i \\right)^2\n\\]\n\nEx.: O erro quadrático médio para o problema de tempo de erupção do gêiser.\n\ndata(\"faithful\")\nhead(faithful)\n\n  eruptions waiting\n1     3.600      79\n2     1.800      54\n3     3.333      74\n4     2.283      62\n5     4.533      85\n6     2.883      55\n\n\n\n# Gráfico do tempo entre as erupções em função do tempo de erupção do gêiseres\nplot(faithful$eruptions, faithful$waiting, pch = 20, ylab=\"Tempo entre Erupções\",\n     xlab = \"Tempo de Erupção\", main = \"Tempo entre as erupções em função do tempo de erupção do gêiser\")\n\n\n\n\n\n\n\n\nPodemos notar que há uma relação linear positiva entre as variáveis. Vamos então ajustar um modelo de regressão linear.\n\nmodelo = lm(faithful$waiting~faithful$eruptions)\nplot(y = faithful$waiting, x = faithful$eruptions, pch = 20, ylab=\"Tempo entre erupções\",\n     xlab = \"Tempo de erupção\", main = \"Tempo entre as erupções em função do tempo de erupção do gêiser\")\nabline(modelo, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nNa reta de regressão temos todos os valores previstos para o tempo de erupção de acordo com os tempos de espera. Podemos então calcular o MSE para o nosso modelo utilizando o comando mse().\n\nmse = sum((teste$waiting-predicao)**2)/nrow(teste)\nmse\n\n[1] 32.41941\n\n\nEntão temos que, em média, o valor estimado para a variável de interesse no conjunto de teste se distancia do valor real observado em 32,41941 escores. Note que esta é uma medida que soma as distâncias ao quadrado, por isso o MSE é um número relativamente grande.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Avaliando Preditores - Introdução ao Pacote Caret</span>"
    ]
  },
  {
    "objectID": "cross-valid.html#alguns-métodos-de-reamostragem",
    "href": "cross-valid.html#alguns-métodos-de-reamostragem",
    "title": "Cross Validation (Validação Cruzada)",
    "section": "Alguns Métodos de Reamostragem",
    "text": "Alguns Métodos de Reamostragem\nAgora vamos compreender como fatiar os dados para realizarmos a reamostragem. Existem vários métodos possíveis mas vamos nos focar em três: k-fold, repeated k-fold e bootstrap.\n\nK-fold\nEste método consiste em fatiar os dados em k pedaços iguais. Utilizamos um pedaço para o teste e os demais para o treino. Então realizamos esse procedimento k vezes, de modo que em cada repetição um novo pedaço seja utilizado para o teste. Para avaliar o erro nós tiramos a média de todos os erros de todas as replicações.\nExemplo: K-fold com 10 partes:\n\n\n\n\n\nQuanto maior o k escolhido obtemos menos viés, porém mais variância. Em outras palavras, você terá uma estimativa muito precisa do viés entre os valores previstos e os valores verdadeiros, porém altamente variável. Agora quanto menor o k escolhido, mais viés e menos variância. Ou seja, não iremos necessariamente obter uma boa estimativa do viés, mas ela será menos variável.\nOBS: Quando o k é igual ao tamanho da amostra, o método é também conhecido como leave-one-out.\nEx.: vamos utilizar reamostragem por k-fold no conjunto de dados spam.\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(kernlab)\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\ndata(spam)\nnoTreino = createDataPartition(y = spam$type, p = 0.75, list = F)\ntreino = spam[noTreino,]\nteste = spam[-noTreino,]\n# Para fazer a reamostragem por k-fold vamos utilizar o comando createFolds():\nfolds = createFolds(y = spam$type, k = 10, list = T, returnTrain = T)\n\nOs principais argumentos da função createFolds() são:\n\ny = a variável de interesse (no nosso caso, o tipo do e-mail);\nk = o número (inteiro) de partições que você deseja.\nlist = argumento do tipo logical. Se TRUE → os resultados serão mostrados em uma lista, se FALSE → os resultados serão mostrados em uma matriz.\nreturnTrain = argumento do tipo logical. Se TRUE, retorna amostras treino. Se FALSE, retorna amostras teste.\n\nVamos verificar o tamanho de cada partição da nossa amostra treino:\n\nsapply(folds,length)\n\nFold01 Fold02 Fold03 Fold04 Fold05 Fold06 Fold07 Fold08 Fold09 Fold10 \n  4141   4141   4141   4140   4141   4140   4142   4141   4141   4141 \n\n\nAgora vamos fazer o mesmo para a amostra teste:\n\nfolds = createFolds(y = spam$type, k = 10, list = T, returnTrain = F)\nsapply(folds,length)\n\nFold01 Fold02 Fold03 Fold04 Fold05 Fold06 Fold07 Fold08 Fold09 Fold10 \n   460    460    460    461    460    459    460    461    460    460 \n\n\nOutra opção de realizar a reamostragem por k-fold é aplicá-la diretamente na função train.\n\ncontrole = trainControl(method = \"cv\", number = 10)\nmodelo = caret::train(type ~ ., data = spam, method = \"glm\", trControl = controle)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\nRepeated K-fold\nO repeated k-fold se resume a repetir o método k-fold várias vezes, com o objetivo de melhorar nossa reamostragem.\nEx.: Vamos aplicar um método de treino 3 vezes em 10 folds.\n\ncontrole = trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\nmodelo = caret::train(type ~ ., data = spam, method = \"glm\", trControl = controle)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\nBootstrap\nO bootstrap é uma técnica de reamostragem com o propósito de reduzir desvios e realizar amostragem dos dados de treino com repetições. Já vimos anteriormente que este é o método default do comando train(), onde é feito 25 reamostragens por bootstrap.\nEmbora esse seja o padrão podemos alterar através do comando trainControl(). Por exemplo, vamos alterar o número de reamostragens de 25 para 10.\n\ncontrole = trainControl(method = \"boot\", number = 10)\nmodelo = train(type ~ ., data = spam, method = \"glm\", trControl = controle)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nPodemos também realizarmos bootstrap fora da função train(), utilizando o comando createResample().\n\nfolds = createResample(y = spam$type, times = 10, list = F)"
  },
  {
    "objectID": "comp-func.html",
    "href": "comp-func.html",
    "title": "1  Comparando Funções Preditoras",
    "section": "",
    "text": "Como já foi dito em capítulos anteriores, existem diversas formas de comparar preditores. Nesse capítulo, vamos estudar um meio de fazer isso e ver mais detalhadamente as medidas de comparação que o R retorna ao usarmos esse método.\n\n1.0.1 Exemplo de Comparação de Regressores - base faithful\nVamos usar a base de dados faithful já presente no R.\n\ndata(\"faithful\")\n# verificando a estrutura da base\nstr(faithful)\n\n'data.frame':   272 obs. of  2 variables:\n $ eruptions: num  3.6 1.8 3.33 2.28 4.53 ...\n $ waiting  : num  79 54 74 62 85 55 88 85 51 85 ...\n\n\nNote que a base apresenta apenas duas variáveis: eruptions, que contém uma amostra corresponde ao tempo em minutos que o gêiser Old Faithful permanece em erupção e waiting, que contém uma amostra correspondente ao tempo em minutos até a próxima erupção. Vamos tentar prever a variável waiting através da variável eruptions. Note ainda que a variável de interesse é quantitativa contínua, portanto queremos construir um regressor.\nVamos treinar nosso modelo utilizando 3 métodos separadamente: linear model, Projection Pursuit Regression e k-Nearest Neighbor. Para fazer a comparação, vamos colocar a mesma semente antes de cada treino para que todos sejam feitos da mesma forma e assim torne a comparação mais “justa”. Note também que estamos usando toda a base de dados pra treinar o medelo. Isso porque estamos apenas avaliando o melhor modelo.\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n# usando o método de validação cruzada tiramos a dependência da amostra\nTC = trainControl(method=\"repeatedcv\", number=10,repeats=3)\nset.seed(371)\nmodelo_lm = train(waiting~eruptions, data=faithful, method=\"lm\", trControl=TC)\nset.seed(371)\nmodelo_ppr = train(waiting~eruptions, data=faithful, method=\"ppr\", trControl=TC)\nset.seed(371)\nmodelo_knn = train(waiting~eruptions, data=faithful, method=\"knn\", trControl=TC)\n\nAgora, como sabemos qual desses é o melhor modelo para nosso Regressor?\n\nresultados = resamples(list(LM=modelo_lm, PPR=modelo_ppr, KNN=modelo_knn))\nsummary(resultados)\n\n\nCall:\nsummary.resamples(object = resultados)\n\nModels: LM, PPR, KNN \nNumber of resamples: 30 \n\nMAE \n        Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLM  3.816660 4.396526 4.723050 4.792316 5.063279 6.087023    0\nPPR 3.847465 4.329571 4.638090 4.728487 5.133559 5.980745    0\nKNN 3.565922 4.380002 4.717796 4.735160 5.167973 5.909983    0\n\nRMSE \n        Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLM  4.769227 5.375918 5.919905 5.877351 6.204474 7.037539    0\nPPR 4.775950 5.258969 5.871960 5.725215 6.099465 6.865713    0\nKNN 4.564997 5.308376 5.828188 5.773268 6.275956 6.892789    0\n\nRsquared \n         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLM  0.7232859 0.7855436 0.8198045 0.8154236 0.8443912 0.8715797    0\nPPR 0.7461656 0.7964453 0.8243005 0.8241913 0.8567375 0.8812427    0\nKNN 0.7636897 0.7964592 0.8227743 0.8218778 0.8453996 0.8771367    0\n\n\nRepare que foi calculada três diferentes medidas: “MAE”, “RMSE”, e “Rsquared”.\nO Erro Médio Absoluto (MAE - Mean Absolute Error) é dado pelo média dos desvios absolutos. \\[MAE = \\frac{\\sum\\limits_{i=1}^{n}\\mid estimado_i - real_i\\mid}{n}\\quad, i=1,2,...,n.\\]\nA Raiz do Erro Quadrático Médio (RMSE - Root Mean Squared Error), como o nome já diz, não é nada mais que a raiz quadrada do Erro Quadrático Médio já citado no capítulo de [Tipos de Erro]. \\[RMSE=\\sqrt{MSE}=\\sqrt{\\frac{\\sum\\limits_{i=1}^{n} \\left( estimado_i-real_i \\right)^{2}}{n}}\\quad, i=1,2,...,n.\\]\nO Coeficiente de Determinação, Também chamado de \\(R^2\\) (R squared), é dado pela razão entre o MSE e a Variância subtraído de 1. \\[R^2 =1- \\frac{MSE}{Var}= 1-\\frac{\\sum\\limits_{i=1}^{n} (real_i - estimado_i)^2}{\\sum\\limits_{i=1}^{n} (real_i - média)^2}\\quad, i=1,2,...,n.\\]\nPortanto, queremos o modelo que possua MAE e RMSE baixo e \\(R^2\\) alto. Para vizualizar melhor, podemos construir um boxplot comparativo da seguinte forma:\n\n# Ajustando as escalas dos gráficos:\nescala &lt;- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\n# Plotando os dados:\nbwplot(resultados, scales=escala)\n\n\n\n\n\nPelos boxplots podemos perceber que o modelo linear é o que possui a pior mediana nas três medidas comparativas e parece ter os dados mais espalhados, principalmente no �2, o que indica que ele possui alta variabilidade. Quanto ao KNN e o PPR, os dados estão mais concentrados no RMSE e no �2, embora tenham bastante outliers. Parece que o PPR é levemente melhor que o KNN, mas é preciso uma análise mais profunda.\n\nlibrary(lattice)\n# Comparando o comportamento de cada fold nos modelos KNN e PPR\nxyplot(resultados, models=c(\"PPR\", \"KNN\"))\n\n\n\n\nNote que a maior parte dos folds está acima da diagonal, indicando que o KNN tem um erro absoluto médio (MAE) menor que o PPR. Vamos olhar novamente para o cálculo que fizemos mais acima.\n\nresultados = resamples(list(LM=modelo_lm, PPR=modelo_ppr, KNN=modelo_knn))\nsummary(resultados)\n\n\nCall:\nsummary.resamples(object = resultados)\n\nModels: LM, PPR, KNN \nNumber of resamples: 30 \n\nMAE \n        Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLM  3.816660 4.396526 4.723050 4.792316 5.063279 6.087023    0\nPPR 3.847465 4.329571 4.638090 4.728487 5.133559 5.980745    0\nKNN 3.565922 4.380002 4.717796 4.735160 5.167973 5.909983    0\n\nRMSE \n        Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLM  4.769227 5.375918 5.919905 5.877351 6.204474 7.037539    0\nPPR 4.775950 5.258969 5.871960 5.725215 6.099465 6.865713    0\nKNN 4.564997 5.308376 5.828188 5.773268 6.275956 6.892789    0\n\nRsquared \n         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLM  0.7232859 0.7855436 0.8198045 0.8154236 0.8443912 0.8715797    0\nPPR 0.7461656 0.7964453 0.8243005 0.8241913 0.8567375 0.8812427    0\nKNN 0.7636897 0.7964592 0.8227743 0.8218778 0.8453996 0.8771367    0\n\n\nPodemos notar que o KNN tem uma posição melhor que o PPR em todas as medidas. Como saber se essa diferença é significativa? Vamos calcular as diferenças entre os dois modelos e avaliar atravé do p-valor.\n\n#Calcular diferença entre modelos, e realizar\n#testes de hipótese para as diferenças.\ndiferencas = diff(resultados)\nsummary(diferencas)\n\n\nCall:\nsummary.diff.resamples(object = diferencas)\n\np-value adjustment: bonferroni \nUpper diagonal: estimates of the difference\nLower diagonal: p-value for H0: difference = 0\n\nMAE \n    LM   PPR       KNN      \nLM        0.063829  0.057156\nPPR 0.18           -0.006673\nKNN 1.00 1.00               \n\nRMSE \n    LM       PPR      KNN     \nLM            0.15214  0.10408\nPPR 0.002181          -0.04805\nKNN 0.422993 0.946570         \n\nRsquared \n    LM      PPR       KNN      \nLM          -0.008768 -0.006454\nPPR 0.01111            0.002313\nKNN 0.54786 1.00000            \n\n\nObserve que, para cada medida, acima da diagonal temos a diferença entre os modelos e abaixo da diagonal o p-valor do teste de comparação entre eles. Portanto, se considerarmos um nível de significância de 1%, é razoável dizer que os modelos PPR e KKN produzem resultados significativamente diferentes. Sendo assim, escolheriamos o método KNN para treinar nosso modelo."
  },
  {
    "objectID": "erros-amostrais.html",
    "href": "erros-amostrais.html",
    "title": "5  Erros Amostrais",
    "section": "",
    "text": "5.1 Erro dentro da Amostra (In Sample Error)\nA taxa de erro dentro da amostra refere-se ao erro calculado no mesmo conjunto de dados utilizado para treinar o modelo preditivo. Na literatura, isso é frequentemente denominado como “erro de resubstituição”. Em outras palavras, essa taxa de erro mede o quanto algoritmo de previsão se ajusta exatamente aos mesmos dados utilizados para o treinamento do modelo. No entanto, quando o modelo é aplicado a um novo conjunto de dados, é esperado que essa precisão diminua.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Erros Amostrais</span>"
    ]
  }
]