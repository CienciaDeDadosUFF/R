# Avaliando Preditores - Introdução ao Pacote Caret

O pacote caret (abreviação de *Classification And Regression Training*) é um pacote muito útil para apricar os métodos de *aprendizado de máquinas* pois envolve algoritmos que possibilitam que as previsões sejam feitas de forma mais prática, simplificando o processo de criação de modelos preditivos. Neste [guia detalhado](http://topepo.github.io/caret/index.html) pode ser encontrado mais informações sobre o pacote.

## **Avaliando Classificadores**

Vamos utilizar a base de dados spam novamente para realizarmos o procedimento de predição para um e-mail (se ele é spam ou não spam), dessa vez utilizando o pacote caret.

Para fazer a separação da amostra em treino e teste, vamos particionar a base de dados com a função createDataPartition(). Em problemas de classificação, a função createDataPartition() garante que a mesma proporção de cada classe observada no banco de dados seja mantida nas amostras de treino e teste. Ou seja, se o banco de dados tiver 70% de elementos do tipo A e 30% do tipo B, as amostras treino e teste terão cada uma 70% de elementos do tipo A e 30% do tipo B.

Para problemas de regressão, createDataPartition() cria uma partição estratificada com base na variável dependente contínua, dividindo os dados de forma a representar bem a distribuição dos valores da variável de resposta em ambas as amostras de treino e teste.   






```{r}
library(caret)
library(kernlab)
data(spam)
set.seed(371)
noTreino = createDataPartition(y = spam$type, p = 0.75, list = F)
```






Essa função retorna os números das linhas a serem selecionadas para o treino. Os principais argumentos são:

-   **y** = é a variável resposta; será mantida a mesma proporção de cada classe nos conjuntos treino e teste. Para o nosso exemplo, será a variável type. Sendo assim, tanto no treino como no teste teremos a mesma proporção de e-mails spam e não spam.

-   **p** = porcentagem da amostra que será utilizada para o treino. Para o nosso exemplo, escolhemos 75%, mas a decisão final é do pesquisador.

-   **list** = argumento do tipo *logical*, se TRUE → os resultados serão mostrados em uma lista, se FALSE → os resultados serão mostrados em uma matriz.

> **OBS:** Esse comando deve ser utilizado apenas quando os dados são amostras independentes.

Agora vamos separar as amostras que irão para o conjunto de treino e as que irão para o conjunto de teste.






```{r}
# Separando as linhas para o treino:
treino = spam[noTreino,]
# Separando as linhas para o teste:
teste = spam[-noTreino,]
```






Após feita a separação das amostras treino e teste, podemos realizar o treinamento do modelo. Para isso é preciso escolher um dos modelos para ser utilizado. Uma lista com todos os modelos implementados no pacote caret pode ser vista com o seguinte comando:






```{r}
names(getModelInfo())
```






Para o nosso exemplo, vamos utilizar regressão logística, que possui no caret a alcunha "glm" (*generalized linear model*).

Agora vamos criar o nosso modelo, utilizando apenas a amostra **treino**. Para isso vamos usar o comando train().






```{r}
modelo = train(type ~ ., data = treino, method = "glm")

```






No primeiro argumento colocamos qual variável estamos tentando prever em função de qual(is). No nosso caso, queremos prever "type" em função ("\~") de todas as outras, por isso utilizamos o ponto ".". Em seguida dizemos de qual base de dados queremos construir o modelo e por último o método de treinamento utilizado.

:::{.callout-tip}
Caso deseje utilizar apenas algumas das variáveis explicativas, basta detalhar quais variáveis devem ser consideradas no modelo, como o exemplo abaixo:





```{r}
modelo2 = train(type ~ your+money+capitalAve, data = treino, method = "glm")

```





:::


Agora vamos dar uma olhada como ficou nosso modelo.






```{r}
modelo
```






Podemos observar que utilizamos uma amostra de tamanho 3451 no treino e 57 variáveis para prever a qual classe um e-mail pertence, spam ou não spam. O que a função faz é ajustar o modelo selecionado (glm no nosso caso) nos dados de treino, buscando otimizar uma métrica pré determinada (o padrão para classificação é acurácia). Quando o modelo necessita de ajuste de hiperparâmetros, a função utiliza métodos de reamostragem para selecionar o melhor modelo. O padrão é reamostragem por bootstrapping com 25 replicações. Trataremos deste tema posteriormente.

Uma vez que ajustamos o modelo podemos aplicá-lo na amostra **teste**, para estimarmos a precisão do classificador. Para isso utilizamos o comando predict(). Dentro da função nós passamos o modelo que ajustamos no treino e em qual base de dados gostaríamos de realizar a predição.






```{r}
predicao = predict(modelo, newdata = teste)
head(predicao, n=30)

```






Ao fazermos isso obtemos uma série de predições para as classes dos e-mails do conjunto teste. Podemos então realizar a avaliação do modelo comparando os resultados da predição com as reais classes dos e-mails, por meio do comando confusionMatrix().

### **Matriz de Confusão *(Confusion Matrix)***

A matriz de confusão é a matriz de comparação feita após a predição, onde as linhas correspondem ao que foi previsto e as colunas correspondem à verdade conhecida.

*Exemplo:* A matriz de confusão para o problema de predição dos e-mails em spam ou não spam fica da seguinta forma:

![](https://cienciadedadosuff.github.io/assets/r/courses/machine_learning/01/images/Matriz%20de%20Confusao%20Vazia.png)

Onde na primeira coluna se encontram os elementos que possuem a característica de interesse (os e-mails que são spam), e, respectivamente nas linhas, os que foram corretamente identificados - o qual são chamados de **Verdadeiros Positivos (VP)** - e os que foram erroneamente identificados - os **Falsos Negativos (FP)**. Na segunda coluna se encontram os elementos que não possuem a característica de interesse (os e-mails que são ham) e, respectivamente nas linhas, os que foram erroneamente identificados - o qual são chamados de **Falsos Positivos (FN)** - e os que foram corretamente identificados - os **Verdadeiros Negativos (VN)**.

Com as devidas classificações a matriz de confusão fica da seguinte forma:

![](https://cienciadedadosuff.github.io/assets/r/courses/machine_learning/01/images/Matriz%20de%20Confusao%20Preenchida.png)

Dentro da função passamos as predições que obtemos pelo modelo ajustado e as reais classificações dos e-mails do conjunto teste.






```{r}
confusionMatrix(predicao, teste$type)

```






A função retorna a matriz de confusão e alguns dados estatísticos, como por exemplo a Precisão (Accuracy), o Intervalo de Confiança com 95% de confiança (95% CI), a Sensibilidade (Sensitivity), Especificidade (Specificity), entre outros.

Podemos notar que o GLM foi um bom modelo de treinamento para os nossos dados pois obtivemos altas taxas de acertos: uma precisão de 0,94, 0,96 de sensitividade e 0,90 de especificidade. Vamos ver melhor algumas dessas estatísticas:

**Definição (Sensibilidade):** A *sensibilidade* de um método de predição é a porcentagem dos elementos da amostra que **possuem** a característica de interesse e foram corretamente identificados. Para o nosso exemplo dos e-mails, a sensabilidade é a porcentagem dos e-mails que são spam e foram classificados pelo nosso algoritmo de predição como spam.

Ou seja, podemos escrever $Sensibilidade = \frac{VP}{VP+FN}$

**Definição (Especificidade):** A *especificidade* de um método de predição é a porcentagem dos elementos da amostra que **não** possuem a característica de interesse e foram corretamente identificados. Para o nosso exemplo dos e-mails, a especificidade é a porcentagem dos e-mails que são "ham" e o algoritmo de predição os classificou como tal.

Ou seja, podemos escrever $Especificidade = \frac{VN}{VN+FP}$

Quando obtemos as sensibilidades e as especificidades de diferentes preditores, naturalmente surge o questionamente: qual deles é melhor para estimar as verdadeiras características de interesse? A resposta depende do que é mais importante para o problema.

Se identificar corretamente os positivos for mais importante, utilizamos o preditor com maior sensibilidade. Se identificar corretamente os negativos for mais importante, utilizamos o preditor com maior especificidade.

Outra medida para avaliar a qualidade do nosso preditor é a **precisão** (*Accuracy*). Ela avalia a porcentagem de acertos que tivemos em geral. Ou seja, somamos o número de Verdadeiros Positivos com o número de Verdadeiros Negativos e dividimos pelo tamanho da amostra. $Precisão = \frac{VP+VN}{VP+VN+FN+FP}$

Para demais medidas da matriz de confusão consulte o \[apêndice\].

## **Avaliando Regressores**

Agora vamos utilizar a base de dados `faithful` para tentar prever o tempo de espera (*waiting*) entre uma erupção e outra de um gêiser dado a duração das erupções (*eruption*).






```{r}
data("faithful")
head(faithful)
```






Primeiro, vamos separar a amostra em treino e teste.






```{r}
set.seed(39)
noTreino = createDataPartition(y=faithful$waiting, p=0.7, groups = 5, list=F)
treino = faithful[noTreino,]; teste = faithful[-noTreino,]
```






Quando o argumento `y` é numérico, a amostra é dividida em grupos com base nos percentis e é feita uma amostragem estratificada. O número de percentis é definido pelo argumento `groups`.

Agora temos que treinar nosso modelo. Para esse exemplo vamos usar a Regressão Linear (*LM - Linear Regression*).

> Os métodos disponíveis e seus usos podem ser encontrados no [guia do caret](https://topepo.github.io/caret/available-models.html).

Vamos treinar nosso modelo utilizando a amostra **treino**.






```{r}
modelo = caret::train(waiting~eruptions, data = treino, method = "lm")
```






Novamente, colocamos a variável que tentamos prever em função das outras. No caso, só temos duas variáveis então não precisamos colocar o ponto como no \[classificador\].






```{r}
modelo
```






Podemos ver que temos 192 observações no conjunto treino e 1 preditor.

Agora vamos aplicar nosso modelo na amostra teste para avaliar o erro dele.






```{r}
predicao = predict(modelo, newdata = teste)

```






Assim como no classificador, a função `predict` nos retorna a previsão dos tempos entre as erupções dado os tempos das durações das erupções.

### **MSE**

Assim como há diversas formas de compararmos a qualidade dos classificadores, há também diversas formas de compararmos regressores. O que estudaremos agora é o MSE (*mean squared error* - erro quadrático médio). Mais formas de comparação de regressores também serão vistas futuramente.

O MSE é a média de quanto os valores previstos para as observações se distanciaram dos valores verdadeiros dessa observação. Obtemos ele somando essas distâncias entre os valores previstos e os reais ao quadrado e dividindo por n.

$$
MSE = \frac{1}{n}\sum_{i=1}^{n} \left( Yreal_i - Yestimado_i \right)^2
$$

![](https://cienciadedadosuff.github.io/assets/r/courses/machine_learning/01/images/MSE.jpg){alt="tela_0"}

**Ex.:** O erro quadrático médio para o problema de tempo de erupção do gêiser.






```{r}
data("faithful")
head(faithful)

```

```{r}
# Gráfico do tempo entre as erupções em função do tempo de erupção do gêiseres
plot(faithful$eruptions, faithful$waiting, pch = 20, ylab="Tempo entre Erupções",
     xlab = "Tempo de Erupção", main = "Tempo entre as erupções em função do tempo de erupção do gêiser")
```






Podemos notar que há uma relação linear positiva entre as variáveis. Vamos então ajustar um modelo de regressão linear.






```{r}
modelo = lm(faithful$waiting~faithful$eruptions)
plot(y = faithful$waiting, x = faithful$eruptions, pch = 20, ylab="Tempo entre erupções",
     xlab = "Tempo de erupção", main = "Tempo entre as erupções em função do tempo de erupção do gêiser")
abline(modelo, col = "red", lwd = 2)

```






Na reta de regressão temos todos os valores previstos para o tempo de erupção de acordo com os tempos de espera. Podemos então calcular o MSE para o nosso modelo utilizando o comando mse().






```{r}
mse = sum((teste$waiting-predicao)**2)/nrow(teste)
mse
```






Então temos que, em média, o valor estimado para a variável de interesse no conjunto de teste se distancia do valor real observado em 32,41941 escores. Note que esta é uma medida que soma as distâncias ao quadrado, por isso o MSE é um número relativamente grande.

